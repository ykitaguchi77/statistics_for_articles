{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBbnhqyJTE7SNEbORImeL9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykitaguchi77/statistics_for_articles/blob/main/Blepharoptosis_dryeye_20250603.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9uCOYWtH2J1"
      },
      "outputs": [],
      "source": [
        "# prompt: gdrive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colabで動作する完全版スクリプト（日本語コメント付き）\n",
        "# 保存先: /content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ.xlsx\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.contingency_tables import Table2x2\n",
        "\n",
        "# Excelファイルの読み込み\n",
        "file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "\n",
        "# ΔMRD-1を追加\n",
        "df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "\n",
        "# 術前SPKが無い症例のみ抽出\n",
        "df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "\n",
        "# ===============================\n",
        "# Figure 1A：術後モデルのROC曲線\n",
        "# ===============================\n",
        "# 使用する変数（欠損除外）\n",
        "post_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"MRD-2 pre\", \"BUT pre\", \"BUT post\"]\n",
        "df_post = df[[\"SPK post\"] + post_cols].dropna()\n",
        "\n",
        "X_post = sm.add_constant(df_post[post_cols])\n",
        "y_post = df_post[\"SPK post\"]\n",
        "\n",
        "# ロジスティック回帰モデルの構築\n",
        "post_model = sm.Logit(y_post, X_post).fit(disp=False)\n",
        "\n",
        "# 予測確率とROC曲線\n",
        "y_prob_post = post_model.predict(X_post)\n",
        "fpr_p, tpr_p, _ = roc_curve(y_post, y_prob_post)\n",
        "auc_p = roc_auc_score(y_post, y_prob_post)\n",
        "\n",
        "# グラフ出力\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr_p, tpr_p, label=f\"AUC = {auc_p:.3f}\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"1 - Specificity\")\n",
        "plt.ylabel(\"Sensitivity\")\n",
        "plt.title(\"Figure 1A  ROC - Postoperative Model\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Fig1A_ROC_postop.png\", dpi=350)\n",
        "\n",
        "# ===============================\n",
        "# Figure 1B：スコアモデルのROC曲線\n",
        "# ===============================\n",
        "# スコア定義関数\n",
        "def calc_score(row):\n",
        "    score = 0\n",
        "    if row[\"MRD-1 pre\"] < 0.5:\n",
        "        score += 2\n",
        "    if row[\"ΔMRD-1\"] >= 2.0:\n",
        "        score += 1\n",
        "    if row[\"BUT pre\"] < 5:\n",
        "        score += 1\n",
        "    if row[\"levator_function pre\"] < 8:\n",
        "        score += 1\n",
        "    return score\n",
        "\n",
        "df[\"score\"] = df.apply(calc_score, axis=1)\n",
        "df_score = df[[\"score\", \"SPK post\"]].dropna()\n",
        "\n",
        "# モデル構築とROC\n",
        "X_score = df_score[[\"score\"]]\n",
        "y_score = df_score[\"SPK post\"]\n",
        "model_score = LogisticRegression().fit(X_score, y_score)\n",
        "y_prob_score = model_score.predict_proba(X_score)[:, 1]\n",
        "fpr_s, tpr_s, _ = roc_curve(y_score, y_prob_score)\n",
        "auc_s = roc_auc_score(y_score, y_prob_score)\n",
        "\n",
        "# グラフ出力\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(fpr_s, tpr_s, label=f\"AUC = {auc_s:.3f}\", color=\"darkorange\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel(\"1 - Specificity\")\n",
        "plt.ylabel(\"Sensitivity\")\n",
        "plt.title(\"Figure 1B  ROC - Preoperative Score Model\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Fig1B_ROC_preop.png\", dpi=350)\n",
        "\n",
        "# ===============================\n",
        "# Figure 2：フォレストプロット（四分位OR）\n",
        "# ===============================\n",
        "# 対象変数\n",
        "predictors = [\"MRD-1 pre\", \"ΔMRD-1\", \"BUT pre\", \"BUT post\", \"levator_function pre\", \"MRD-2 pre\"]\n",
        "\n",
        "forest_data = []\n",
        "\n",
        "for var in predictors:\n",
        "    df_q = df[[var, \"SPK post\"]].dropna().copy()\n",
        "    df_q[\"quartile\"] = pd.qcut(df_q[var].rank(method=\"first\"), 4, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
        "\n",
        "    q_vals = df_q[var].quantile([0, 0.25, 0.5, 0.75, 1]).values\n",
        "    ranges = {\n",
        "        \"Q1\": f\"{q_vals[0]:.2f}–{q_vals[1]:.2f}\",\n",
        "        \"Q2\": f\"{q_vals[1]:.2f}–{q_vals[2]:.2f}\",\n",
        "        \"Q3\": f\"{q_vals[2]:.2f}–{q_vals[3]:.2f}\",\n",
        "        \"Q4\": f\"{q_vals[3]:.2f}–{q_vals[4]:.2f}\"\n",
        "    }\n",
        "\n",
        "    # 基準群（リスク最小）を指定\n",
        "    ref_q = \"Q4\" if var != \"ΔMRD-1\" else \"Q1\"\n",
        "\n",
        "    for q in [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]:\n",
        "        if q == ref_q:\n",
        "            forest_data.append((var, q, ranges[q], 1.0, 1.0, 1.0))\n",
        "        else:\n",
        "            a = ((df_q[\"quartile\"] == q) & (df_q[\"SPK post\"] == 1)).sum()\n",
        "            b = ((df_q[\"quartile\"] == q) & (df_q[\"SPK post\"] == 0)).sum()\n",
        "            c = ((df_q[\"quartile\"] == ref_q) & (df_q[\"SPK post\"] == 1)).sum()\n",
        "            d = ((df_q[\"quartile\"] == ref_q) & (df_q[\"SPK post\"] == 0)).sum()\n",
        "            # 0セル補正\n",
        "            if 0 in [a, b, c, d]:\n",
        "                a += 0.5; b += 0.5; c += 0.5; d += 0.5\n",
        "            table = Table2x2([[a, b], [c, d]])\n",
        "            or_val = table.oddsratio\n",
        "            ci_low, ci_high = table.oddsratio_confint()\n",
        "            forest_data.append((var, q, ranges[q], or_val, ci_low, ci_high))\n",
        "\n",
        "# データフレーム化\n",
        "forest_df = pd.DataFrame(forest_data, columns=[\"Variable\", \"Quartile\", \"Range\", \"OR\", \"CI_low\", \"CI_high\"])\n",
        "forest_df[\"Label\"] = forest_df[\"Variable\"] + \" \" + forest_df[\"Quartile\"] + \"\\n(\" + forest_df[\"Range\"] + \")\"\n",
        "\n",
        "# プロット\n",
        "plt.figure(figsize=(7, len(forest_df) * 0.4))\n",
        "y_pos = np.arange(len(forest_df))[::-1]\n",
        "plt.errorbar(forest_df[\"OR\"], y_pos,\n",
        "             xerr=[forest_df[\"OR\"] - forest_df[\"CI_low\"], forest_df[\"CI_high\"] - forest_df[\"OR\"]],\n",
        "             fmt='o', color='navy', ecolor='skyblue', capsize=3)\n",
        "plt.axvline(1, color='gray', linestyle='--')\n",
        "plt.yticks(y_pos, forest_df[\"Label\"])\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Odds Ratio (log scale)\")\n",
        "plt.title(\"Figure 2  Forest Plot – Quartile-based OR\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"Fig2_Forest.png\", dpi=350)\n"
      ],
      "metadata": {
        "id": "c8x3aXGkWKML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：Google Colab用フルスクリプト Ver.Corrected_3M\n",
        "# （日本語コメント付き・350 dpiで図を保存・Forest Plot p値・単変量結果・スコアリング追加）\n",
        "# =============================================================\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 0) Google Drive をマウント（既にマウント済ならスキップ）\n",
        "# ───────────────────────────────────────────\n",
        "# from google.colab import drive\n",
        "# try:\n",
        "#     drive.mount('/content/drive')\n",
        "#     print(\"Google Drive mounted successfully.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error mounting Google Drive: {e}\")\n",
        "#     # exit() # 必要なら中断\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 1) ライブラリ読み込み\n",
        "# ───────────────────────────────────────────\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.contingency_tables import Table2x2\n",
        "from scipy.stats import shapiro, mannwhitneyu, ttest_ind, fisher_exact\n",
        "from statsmodels.formula.api import logit\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "\n",
        "# 計算中の警告を抑制\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "warnings.filterwarnings('ignore', message=\"Method 'first' requires numeric dtype to function correctly\")\n",
        "\n",
        "print(\"Libraries loaded.\")\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 2) Excel 読み込みと基本前処理\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 2. Loading and Basic Preprocessing ---\")\n",
        "# ★ 注意: パス、シート名を確認してください\n",
        "file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "sheet_name = \"対象症例sides\"\n",
        "output_dir = \".\" # 図の保存先フォルダ\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=sheet_name, header=0)\n",
        "    print(f\"Excel file '{os.path.basename(file_path)}' (Sheet: '{sheet_name}') loaded successfully.\")\n",
        "    print(f\"Original data shape: {df_all.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Excel file: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 主要カラムを数値型に変換 ---\n",
        "print(\"Converting key columns to numeric...\")\n",
        "# ★★★ 'MRD-1 post', 'MRD-2 post' を 'MRD-1 3M', 'MRD-2 3M' に修正 ★★★\n",
        "cols_to_convert = [\n",
        "    \"MRD-1 pre\", \"MRD-1 3M\",\n",
        "    \"MRD-2 pre\", \"MRD-2 3M\",\n",
        "    \"levator_function pre\",\n",
        "    \"BUT pre\", \"BUT post\",\n",
        "    \"SPK pre\", \"SPK post\"\n",
        "]\n",
        "converted_cols = []\n",
        "missing_cols = []\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns:\n",
        "        df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "        converted_cols.append(col)\n",
        "    else:\n",
        "        print(f\" - Warning: Column '{col}' not found, skipping conversion.\")\n",
        "        missing_cols.append(col)\n",
        "\n",
        "if \"SPK pre\" not in converted_cols or \"SPK post\" not in converted_cols:\n",
        "     print(f\"Error: Required outcome columns ('SPK pre' or 'SPK post') are missing or failed conversion. Cannot proceed.\")\n",
        "     exit()\n",
        "\n",
        "# --- ΔMRD を計算 ---\n",
        "try:\n",
        "    # ★★★ 計算に 'MRD-1 3M', 'MRD-2 3M' を使用するように修正 ★★★\n",
        "    if \"MRD-1 3M\" in df_all.columns and \"MRD-1 pre\" in df_all.columns:\n",
        "        df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "        print(\"ΔMRD-1 calculated (using MRD-1 3M).\")\n",
        "    else:\n",
        "        print(\"Warning: Could not calculate ΔMRD-1 (MRD-1 3M or MRD-1 pre missing).\")\n",
        "        df_all[\"ΔMRD-1\"] = np.nan\n",
        "\n",
        "    if \"MRD-2 3M\" in df_all.columns and \"MRD-2 pre\" in df_all.columns:\n",
        "        df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "        print(\"ΔMRD-2 calculated (using MRD-2 3M).\")\n",
        "    else:\n",
        "        print(\"Warning: Could not calculate ΔMRD-2 (MRD-2 3M or MRD-2 pre missing).\")\n",
        "        df_all[\"ΔMRD-2\"] = np.nan\n",
        "except Exception as e:\n",
        "    print(f\"Error during ΔMRD calculation: {e}\")\n",
        "    df_all[\"ΔMRD-1\"] = np.nan\n",
        "    df_all[\"ΔMRD-2\"] = np.nan\n",
        "\n",
        "# --- 術前SPKなし症例のみ解析対象 ('df' を作成) ---\n",
        "df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "print(f\"Filtered data for 'SPK pre == 0'. Shape of 'df': {df.shape}\")\n",
        "if df.empty:\n",
        "    print(\"Error: No data remaining after filtering for 'SPK pre == 0'. Cannot proceed.\")\n",
        "    exit()\n",
        "\n",
        "# --- SPK post の存在確認 ---\n",
        "if \"SPK post\" not in df.columns:\n",
        "    print(\"Error: 'SPK post' column not found in the filtered data 'df'.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 5) Figure 2：フォレストプロット（四分位 OR）\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 5. Generating Figure 2: Forest plot (Quartile OR) ---\")\n",
        "# ★★★ predictors リストを '3M' に修正 ★★★\n",
        "predictors = [\"MRD-1 pre\", \"ΔMRD-1\", \"MRD-1 3M\", \"MRD-2 pre\", \"ΔMRD-2\", \"MRD-2 3M\",\n",
        "              \"levator_function pre\", \"BUT pre\", \"BUT post\"]\n",
        "\n",
        "forest_rows = []\n",
        "forest_df = pd.DataFrame() # 初期化\n",
        "missing_cols_forest = [col for col in predictors + [\"SPK post\"] if col not in df.columns]\n",
        "if missing_cols_forest:\n",
        "    print(f\"Error for Figure 2: Missing columns in 'df': {', '.join(missing_cols_forest)}\")\n",
        "else:\n",
        "    df_forest_filt = df[df[\"SPK post\"].isin([0, 1])].copy()\n",
        "    df_forest_filt[\"SPK post\"] = df_forest_filt[\"SPK post\"].astype(int)\n",
        "    print(f\"Data for Forest Plot: {df_forest_filt.shape}\")\n",
        "\n",
        "    for var in predictors:\n",
        "        if var not in df_forest_filt.columns: continue\n",
        "        sub = df_forest_filt[[var, \"SPK post\"]].dropna()\n",
        "        print(f\"Processing {var}: {len(sub)} non-NaN rows.\")\n",
        "        if len(sub) < 4 or sub[var].nunique() < 2: continue\n",
        "\n",
        "        try:\n",
        "            n_quantiles = min(4, sub[var].nunique())\n",
        "            if n_quantiles < 2: continue\n",
        "            labels = [f\"Q{i+1}\" for i in range(n_quantiles)]\n",
        "            sub[\"q\"] = pd.qcut(sub[var].rank(method='first'), n_quantiles, labels=labels, duplicates='drop')\n",
        "        except Exception as e:\n",
        "            print(f\" - Failed quartile creation for {var}: {e}.\")\n",
        "            continue\n",
        "\n",
        "        existing_quartiles = sub[\"q\"].cat.categories.tolist()\n",
        "        ranges = {q_label: f\"{sub[sub['q'] == q_label][var].min():.2f}–{sub[sub['q'] == q_label][var].max():.2f}\"\n",
        "                  if sub[sub['q'] == q_label][var].min() != sub[sub['q'] == q_label][var].max()\n",
        "                  else f\"{sub[sub['q'] == q_label][var].min():.2f}\"\n",
        "                  for q_label in existing_quartiles}\n",
        "\n",
        "        # ★★★ 参照カテゴリ決定ロジックを '3M' に修正 ★★★\n",
        "        ref_q = \"Q1\" if var in [\"ΔMRD-1\", \"MRD-2 pre\", \"ΔMRD-2\", \"MRD-2 3M\"] else existing_quartiles[-1]\n",
        "        if ref_q not in existing_quartiles: ref_q = existing_quartiles[-1]\n",
        "        print(f\" - Ref quartile for {var}: {ref_q}\")\n",
        "\n",
        "        for q in existing_quartiles:\n",
        "            range_str = ranges.get(q, \"N/A\")\n",
        "            if q == ref_q:\n",
        "                forest_rows.append((var, q, range_str, 1.0, 1.0, 1.0, np.nan))\n",
        "                continue\n",
        "            a_orig = ((sub[\"q\"]==q) & (sub[\"SPK post\"]==1)).sum()\n",
        "            b_orig = ((sub[\"q\"]==q) & (sub[\"SPK post\"]==0)).sum()\n",
        "            c_orig = ((sub[\"q\"]==ref_q) & (sub[\"SPK post\"]==1)).sum()\n",
        "            d_orig = ((sub[\"q\"]==ref_q) & (sub[\"SPK post\"]==0)).sum()\n",
        "            a, b, c, d = a_orig+0.5, b_orig+0.5, c_orig+0.5, d_orig+0.5\n",
        "            or_val, ci_low, ci_high, p_val = np.nan, np.nan, np.nan, np.nan\n",
        "            try:\n",
        "                table = Table2x2([[a, b], [c, d]])\n",
        "                or_val = table.oddsratio; ci_low, ci_high = table.oddsratio_confint()\n",
        "                if np.sum([[a_orig, b_orig], [c_orig, d_orig]]) > 0:\n",
        "                    _, p_val = fisher_exact([[a_orig, b_orig], [c_orig, d_orig]], alternative='two-sided')\n",
        "            except Exception as e: pass\n",
        "            forest_rows.append((var, q, range_str, or_val, ci_low, ci_high, p_val))\n",
        "\n",
        "    if forest_rows:\n",
        "        forest_df = pd.DataFrame(forest_rows, columns=[\"Variable\", \"Quartile\", \"Range\", \"OR\", \"CI_low\", \"CI_high\", \"P_value\"])\n",
        "        def create_label(row): return f\"{row['Variable']} {row['Quartile']}\\n({row['Range']})\"\n",
        "        forest_df[\"Label\"] = forest_df.apply(create_label, axis=1)\n",
        "\n",
        "        plt.figure(figsize=(8, max(5, len(forest_df) * 0.45)))\n",
        "        y_pos = np.arange(len(forest_df))[::-1]\n",
        "        plot_data = forest_df.dropna(subset=['OR', 'CI_low', 'CI_high']).copy()\n",
        "        y_pos_plot = y_pos[forest_df[['OR', 'CI_low', 'CI_high']].notna().all(axis=1)]\n",
        "\n",
        "        if not plot_data.empty:\n",
        "            clip_min, clip_max = 1e-3, 1e3\n",
        "            plot_data['OR_plot'] = np.clip(plot_data['OR'], clip_min, clip_max)\n",
        "            plot_data['CI_low_plot'] = np.clip(plot_data['CI_low'], clip_min, clip_max)\n",
        "            plot_data['CI_high_plot'] = np.clip(plot_data['CI_high'], clip_min, clip_max)\n",
        "            xerr_low = np.maximum(plot_data['OR_plot'] - plot_data['CI_low_plot'], 0)\n",
        "            xerr_high = np.maximum(plot_data['CI_high_plot'] - plot_data['OR_plot'], 0)\n",
        "\n",
        "            plt.errorbar(plot_data['OR_plot'], y_pos_plot, xerr=[xerr_low, xerr_high],\n",
        "                         fmt='o', color='navy', ecolor='skyblue', capsize=4, markersize=5, zorder=10)\n",
        "            plt.axvline(1, color='gray', linestyle='--', linewidth=1, zorder=5)\n",
        "            plt.yticks(y_pos, forest_df[\"Label\"], fontsize=9)\n",
        "            plt.xscale(\"log\")\n",
        "            plt.xlabel(\"Odds Ratio (log scale)\")\n",
        "            plt.title(\"Figure 2  Forest plot – Quartile Odds Ratios for Post-op SPK\")\n",
        "            plt.grid(axis='x', linestyle=':', alpha=0.7)\n",
        "            plt.tight_layout(rect=[0, 0.02, 1, 0.97])\n",
        "            fig2_path = os.path.join(output_dir, \"Fig2_Forest.png\")\n",
        "            plt.savefig(fig2_path, dpi=350)\n",
        "            print(f\"Figure 2 saved as {fig2_path}\")\n",
        "            plt.show()\n",
        "        else: print(\"Forest plot not generated (no valid data).\")\n",
        "    else: print(\"Forest plot not generated (no data processed).\")\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 6) Table 1：基本統計量\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 6. Generating Table 1: Baseline characteristics ---\")\n",
        "# ★ Table 1 に含める変数リスト (predictors リストを使用)\n",
        "table1_cols = predictors\n",
        "missing_cols_table1 = [col for col in table1_cols if col not in df.columns]\n",
        "if missing_cols_table1:\n",
        "     print(f\"Error for Table 1: Missing columns in 'df': {', '.join(missing_cols_table1)}\")\n",
        "else:\n",
        "    table1 = df[table1_cols].describe(percentiles=[.25, .5, .75]).T\n",
        "    table1 = table1.rename(columns={\"count\": \"N\", \"50%\": \"Median\", \"25%\": \"Q1\", \"75%\": \"Q3\",\n",
        "                                     \"mean\": \"Mean\", \"std\": \"SD\", \"min\": \"Min\", \"max\": \"Max\"})\n",
        "    table1[\"Missing\"] = df[table1_cols].isna().sum()\n",
        "    table1[\"Total N (SPK pre=0)\"] = len(df)\n",
        "    table1 = table1[[\"Total N (SPK pre=0)\", \"N\", \"Missing\", \"Mean\", \"SD\", \"Min\", \"Q1\", \"Median\", \"Q3\", \"Max\"]]\n",
        "    print(\"\\n=== Table 1: Summary statistics (for subjects with SPK pre = 0) ===\")\n",
        "    display(table1.round(2))\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 7) 正規性検定とヒストグラム\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 7. Normality tests and Histograms ---\")\n",
        "# ★ 正規性検定対象リスト (predictors リストを使用)\n",
        "normality_cols = predictors\n",
        "missing_cols_norm = [col for col in normality_cols if col not in df.columns]\n",
        "if missing_cols_norm:\n",
        "     print(f\"Error for Normality tests: Missing columns: {', '.join(missing_cols_norm)}\")\n",
        "else:\n",
        "    alpha = 0.05\n",
        "    print(f\"Shapiro-Wilk normality test (alpha = {alpha}):\")\n",
        "    for col in normality_cols:\n",
        "        if col not in df.columns: continue\n",
        "        d = df[col].dropna()\n",
        "        print(f\"{col:<25}\", end=\"\")\n",
        "\n",
        "        if len(d) >= 3:\n",
        "            try:\n",
        "                stat, p_shapiro = shapiro(d)\n",
        "                normality = \"Normal\" if p_shapiro > alpha else \"Non-normal\"\n",
        "                print(f\" Samples={len(d):<4} W={stat:.4f}, p={p_shapiro:.4f} ({normality})\")\n",
        "                plt.figure(figsize=(5, 3))\n",
        "                sns.histplot(d, kde=True, bins=15)\n",
        "                plt.title(f\"Distribution of {col}\\n(Shapiro p={p_shapiro:.3f}, N={len(d)})\")\n",
        "                plt.xlabel(col); plt.ylabel(\"Frequency\"); plt.tight_layout()\n",
        "                plt.show()\n",
        "            except Exception as e: print(f\" Error: {e}\")\n",
        "        elif len(d) > 0:\n",
        "            print(f\" Samples={len(d):<4} Too few samples for Shapiro-Wilk test.\")\n",
        "            plt.figure(figsize=(5, 3)); sns.histplot(d, kde=False, bins=max(1, len(d)))\n",
        "            plt.title(f\"Distribution of {col} (N={len(d)})\")\n",
        "            plt.xlabel(col); plt.ylabel(\"Frequency\"); plt.tight_layout(); plt.show()\n",
        "        else: print(\" No data available.\")\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 8) Table 2：単変量比較 & 多変量ロジスティック\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 8. Table 2: Univariate comparison and Multivariate logistic regression ---\")\n",
        "# ★ 解析に使用する連続変数リスト (predictors リストを使用)\n",
        "continuous_vars = predictors\n",
        "analysis_cols_check = continuous_vars + [\"SPK post\"]\n",
        "df_analysis = None; univar_df = pd.DataFrame(); model_multi = None\n",
        "\n",
        "missing_cols_analysis = [col for col in analysis_cols_check if col not in df.columns]\n",
        "if missing_cols_analysis:\n",
        "     print(f\"Error for Table 2: Missing columns in 'df': {', '.join(missing_cols_analysis)}\")\n",
        "else:\n",
        "    df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "    df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "    print(f\"Created 'df_analysis' for Table 2. Shape: {df_analysis.shape}\")\n",
        "\n",
        "    if df_analysis.empty or df_analysis['SPK post'].nunique() < 2:\n",
        "        print(\"Warning: Skipping Table 2 (Not enough data or only one SPK post class).\")\n",
        "    else:\n",
        "        # === 単変量解析 ===\n",
        "        print(\"\\n=== Univariate Analysis ===\")\n",
        "        results_univar = []\n",
        "        alpha = 0.05\n",
        "        for col in continuous_vars:\n",
        "            if col not in df_analysis.columns: continue\n",
        "            g0 = df_analysis[df_analysis[\"SPK post\"]==0][col].dropna()\n",
        "            g1 = df_analysis[df_analysis[\"SPK post\"]==1][col].dropna()\n",
        "            row = {\"Variable\": col, \"N0\": len(g0), \"Mean0\": g0.mean(), \"SD0\": g0.std(),\n",
        "                   \"N1\": len(g1), \"Mean1\": g1.mean(), \"SD1\": g1.std(),\n",
        "                   \"Test\": \"Skipped\", \"p-value\": np.nan}\n",
        "            if len(g0) >= 3 and len(g1) >= 3:\n",
        "                test_name, p = \"Skipped\", np.nan\n",
        "                try:\n",
        "                    norm0 = shapiro(g0)[1] > alpha; norm1 = shapiro(g1)[1] > alpha\n",
        "                    if norm0 and norm1: stat, p = ttest_ind(g0, g1, equal_var=False); test_name = \"Welch's t\"\n",
        "                    else:\n",
        "                        if g0.nunique() > 1 or g1.nunique() > 1: stat, p = mannwhitneyu(g0, g1, alternative='two-sided'); test_name = \"M-Whitney U\"\n",
        "                        else: test_name = \"Identical\"; p = 1.0\n",
        "                    row[\"Test\"] = test_name; row[\"p-value\"] = p\n",
        "                except Exception: row[\"Test\"] = \"Test Error\"\n",
        "            results_univar.append(row)\n",
        "        univar_df = pd.DataFrame(results_univar)\n",
        "        print(\"\\n--- Univariate Analysis Summary ---\")\n",
        "        display(univar_df.rename(columns={'N0':'N (SPK=0)', 'Mean0':'Mean (0)', 'SD0':'SD (0)',\n",
        "                                           'N1':'N (SPK=1)', 'Mean1':'Mean (1)', 'SD1':'SD (1)'})\n",
        "                [['Variable', 'N (SPK=0)', 'Mean (0)', 'SD (0)', 'N (SPK=1)', 'Mean (1)', 'SD (1)', 'Test', 'p-value']]\n",
        "                .round({'Mean (0)': 2, 'SD (0)': 2, 'Mean (1)': 2, 'SD (1)': 2, 'p-value': 4}))\n",
        "\n",
        "        # === 多変量ロジスティック回帰 ===\n",
        "        print(\"\\n=== Multivariate Logistic Regression ===\")\n",
        "        mv_cols = [\"SPK post\"] + continuous_vars\n",
        "        df_mv = df_analysis[mv_cols].dropna()\n",
        "        print(f\"Data for Multivariate model (after dropna): {df_mv.shape}\")\n",
        "        min_samples_needed = len(continuous_vars) + 1\n",
        "        if len(df_mv) < min_samples_needed or df_mv[\"SPK post\"].nunique() < 2:\n",
        "            print(f\"Warning: Skipping multivariate analysis (Insufficient data: {len(df_mv)}/{min_samples_needed} or one class).\")\n",
        "        else:\n",
        "            try:\n",
        "                y_mv = df_mv[\"SPK post\"]; X_mv_cont = df_mv[continuous_vars]\n",
        "                X_mv = sm.add_constant(X_mv_cont, has_constant='add')\n",
        "                model_multi = sm.Logit(y_mv, X_mv).fit(disp=0)\n",
        "                print(model_multi.summary(title=\"Multivariate Logistic Regression Results\",\n",
        "                                          yname=\"SPK post\", xname=[\"Intercept\"] + continuous_vars))\n",
        "                params = model_multi.params; conf = model_multi.conf_int()\n",
        "                conf['Odds Ratio'] = params; conf.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                print(\"\\n--- Odds Ratios (Multivariate Model) ---\"); display(np.exp(conf).round(3))\n",
        "            except Exception as e: print(f\"Error during multivariate regression: {e}\"); model_multi = None\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 9) AICベースのステップワイズ変数選択\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 9. Stepwise variable selection based on AIC ---\")\n",
        "aic_model = None; selected_vars = []\n",
        "if model_multi is not None and 'df_mv' in locals() and not df_mv.empty:\n",
        "    X_step = df_mv[continuous_vars]; y_step = df_mv[\"SPK post\"]\n",
        "    # (ステップワイズ関数の定義は省略 - 前回のコードと同じ)\n",
        "    def stepwise_logistic_aic(X, y, initial_list=[], verbose=True):\n",
        "        included = list(initial_list); best_aic = np.inf\n",
        "        null_model, null_aic = None, np.inf\n",
        "        try:\n",
        "            X_null = sm.add_constant(pd.DataFrame(np.ones(len(y)), index=y.index, columns=['Intercept']), has_constant='add')\n",
        "            null_model = sm.Logit(y, X_null).fit(disp=0); null_aic = null_model.aic\n",
        "            if verbose: print(f\"Null model AIC = {null_aic:.2f}\")\n",
        "        except Exception as e: print(f\"Error fitting null model: {e}\"); return [], None\n",
        "        if included:\n",
        "            try:\n",
        "                current_model = sm.Logit(y, sm.add_constant(X[included], has_constant='add')).fit(disp=0)\n",
        "                best_aic = current_model.aic; print(f\"Initial model AIC = {best_aic:.2f}\")\n",
        "            except Exception as e: print(f\"Error initial model: {e}\"); included = []; best_aic = null_aic\n",
        "        else: best_aic = null_aic\n",
        "        iter_count, max_iter = 0, 2*len(X.columns)+2\n",
        "        while iter_count < max_iter:\n",
        "            iter_count += 1; changed = False; potential_actions = []\n",
        "            excluded = list(set(X.columns) - set(included))\n",
        "            for new_column in excluded:\n",
        "                try: model=sm.Logit(y,sm.add_constant(X[included+[new_column]],has_constant='add')).fit(disp=0); potential_actions.append((model.aic,new_column,'add'))\n",
        "                except Exception: continue\n",
        "            if len(included) > 0:\n",
        "                for drop_column in included:\n",
        "                    temp_included = [col for col in included if col != drop_column]\n",
        "                    if not temp_included: aic_drop = null_aic\n",
        "                    else:\n",
        "                        try: model=sm.Logit(y,sm.add_constant(X[temp_included],has_constant='add')).fit(disp=0); aic_drop=model.aic\n",
        "                        except Exception: continue\n",
        "                    potential_actions.append((aic_drop, drop_column, 'drop'))\n",
        "            if not potential_actions: break\n",
        "            potential_actions.sort(key=lambda x: x[0])\n",
        "            best_new_aic, best_variable, best_action = potential_actions[0]\n",
        "            if best_new_aic < best_aic - 1e-6:\n",
        "                best_aic=best_new_aic\n",
        "                if best_action=='add': included.append(best_variable)\n",
        "                else: included.remove(best_variable)\n",
        "                changed=True; print(f\"Iter {iter_count}: {best_action.title()} {best_variable:<25} New AIC = {best_aic:.2f}\")\n",
        "            else: break\n",
        "            if not changed: break\n",
        "        print(\"\\n=== Final Selected Model (AIC) ===\")\n",
        "        final_model = None\n",
        "        if included:\n",
        "            try:\n",
        "                final_model=sm.Logit(y,sm.add_constant(X[included],has_constant='add')).fit(disp=0)\n",
        "                print(final_model.summary(title=\"Stepwise AIC Result\",yname=y.name,xname=[\"Intercept\"]+included))\n",
        "                params_final=final_model.params; conf_final=final_model.conf_int(); conf_final['OR']=params_final; conf_final.columns=['CI 2.5%','CI 97.5%','OR']\n",
        "                print(\"\\n--- Odds Ratios (Final AIC Model) ---\"); display(np.exp(conf_final).round(3))\n",
        "            except Exception as e: print(f\"Error final model: {e}\"); final_model=null_model\n",
        "        else: final_model=null_model; print(\"No variables selected.\"); print(final_model.summary())\n",
        "        return included, final_model\n",
        "    try:\n",
        "        selected_vars, aic_model = stepwise_logistic_aic(X_step, y_step, verbose=True)\n",
        "        print(f\"\\nSelected variables by AIC: {selected_vars}\")\n",
        "    except Exception as e: print(f\"Error during stepwise selection: {e}\"); aic_model = None; selected_vars = []\n",
        "else: print(\"Skipping stepwise selection.\")\n",
        "\n",
        "# ───────────────────────────────────────────\n",
        "# 10) Forest plot 用 OR データ表示 (p値含む)\n",
        "# ───────────────────────────────────────────\n",
        "print(\"\\n--- 10. Forest Plot OR and p-value raw data ---\")\n",
        "if 'forest_df' in locals() and not forest_df.empty:\n",
        "    print(\"Displaying the data used for the Forest Plot (Figure 2):\")\n",
        "    display_cols = [\"Variable\", \"Quartile\", \"Range\", \"OR\", \"CI_low\", \"CI_high\", \"P_value\"]\n",
        "    display_cols = [col for col in display_cols if col in forest_df.columns]\n",
        "    display(forest_df[display_cols].round({'OR': 3, 'CI_low': 3, 'CI_high': 3, 'P_value': 4}))\n",
        "else: print(\"No data available to display for the Forest Plot.\")\n",
        "\n",
        "\n",
        "# --- 最後に解析終了メッセージ ---\n",
        "print(\"\\n==========================================================\")\n",
        "print(f\"==== 解析終了：図 (PNG) はフォルダ '{output_dir}' に保存されました ====\")\n",
        "print(\"==========================================================\")"
      ],
      "metadata": {
        "id": "5XSJuIBoWMsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 眼瞼下垂術後SPKリスクスコアリングシステム（術前因子のみ）解説\n",
        "\n",
        "## 1. スコアリングシステムの目的\n",
        "\n",
        "このスコアリングシステムの主な目的は、眼瞼下垂手術を受ける患者さんの**術前の情報のみ**（※注意点あり）を用いて、手術後に**SPK（点状表層角膜症）を発症するリスクを予測すること**です。\n",
        "\n",
        "これにより、術前からリスクが高い患者さんを特定し、術後のドライアイ管理（点眼の強化、フォローアップの頻度調整など）をより重点的に行うための判断材料を提供することを目指しています。\n",
        "\n",
        "## 2. スコアリングの根拠\n",
        "\n",
        "本スコアリングシステムは、実施された統計解析結果、特に**フォレストプロット（Figure 2）**に基づいています。フォレストプロットでは、各因子（例：MRD-1 pre, ΔMRD-1, BUT pre）を値に基づいて四分位に分割し、各グループの術後SPK発症との関連性（オッズ比）を視覚化しています。\n",
        "\n",
        "この結果から、術後SPKリスク上昇と**関連が強い（オッズ比 > 1 かつ p値が比較的小さい）**と判断された因子とその閾値（カットオフ値）を抽出し、スコアリング項目として採用しました。\n",
        "\n",
        "## 3. スコアリングの構成\n",
        "\n",
        "以下の4つの術前（※）リスク因子と、それぞれのカットオフ値に基づいてスコアが計算されます。\n",
        "\n",
        "| リスク因子               | 条件（カットオフ値） | 割り当て点数 | 根拠（フォレストプロット等）                          |\n",
        "| :----------------------- | :----------------- | :--------- | :-------------------------------------------------- |\n",
        "| **MRD-1 pre**            | `< 0.5 mm`         | **2点**    | Q1 (≤0.0), Q2 (=0.5)でORが高い傾向 (p=0.064, 0.034) |\n",
        "| **ΔMRD-1**               | `>= 2.0 mm`        | 1点        | Q2 (≥2.0)以降でORが高い (p=0.097, 0.027, 0.027)     |\n",
        "| **BUT pre**              | `< 5 秒`           | 1点        | Q1(<3), Q2(=3), Q3(<5)でORが高い傾向 (p=0.349, 0.220) |\n",
        "| **levator function pre** | `< 8 mm`           | 1点        | Q1(<8)でORが有意に高い (p=0.008)                     |\n",
        "| **合計スコア**           |                    | **0点 〜 5点** |                                                     |\n",
        "\n",
        "**※ ΔMRD-1 に関する重要な注意点:**\n",
        "現在の計算では、`ΔMRD-1` は `MRD-1 3M - MRD-1 pre` として算出されています。これは**術後3ヶ月時点のデータ**を含むため、厳密には**「術前の情報のみ」を用いたスコアではありません**。この点については以下の解釈が考えられます。\n",
        "\n",
        "1.  **術後予測モデル因子としての評価:** このスコアは術後3ヶ月までの情報を含めた上でのSPKリスク評価モデルの一部とみなす。\n",
        "2.  **目標挙上量の代理:** ΔMRD-1を、術前に計画された、あるいは期待されるMRD-1の変化量の代理指標として扱う。もし術前に目標挙上量を決定している場合、その値で代替すれば純粋な術前スコアとなり得る。\n",
        "\n",
        "このスコアを解釈・利用する際には、この点を十分に理解する必要があります。純粋な「術前予測スコア」を目指す場合、ΔMRD-1を除外するか、術前に決定可能な値（例：目標挙上量）に置き換えるなどの修正を検討する価値があります。\n",
        "\n",
        "**点数配分について:**\n",
        "`MRD-1 pre < 0.5` に2点が割り当てられているのは、フォレストプロット結果（低い四分位でのORの高さ）や、臨床的な重要度（術前MRD-1低値が特に重要）を反映していると考えられます。他の因子はリスク上昇が見られた閾値で1点としています。\n",
        "\n",
        "## 4. スコアの解釈と利用\n",
        "\n",
        "*   **リスク層別化:** 計算された合計スコア（0〜5点）が高いほど、術後SPKを発症するリスクが高いことを示唆します。\n",
        "*   **発生率の目安:** スコア別のSPK発生率（解析結果の表やグラフ参照）は、各スコアを持つ患者群における実際のSPK発症割合の目安となります（例：「スコア4点の群では40.0%がSPKを発症」）。\n",
        "*   **予測性能 (AUC):** ROC曲線下面積（AUC）は、スコアシステム全体の予測精度を示します。今回の解析ではAUC=0.659であり、これは中程度の識別能力（modest discriminative ability）を示唆します（AUC 1.0が完璧な予測、0.5がランダム）。\n",
        "*   **カットオフ値:** Youden Indexに基づく最適なカットオフ値（解析結果では「≥ 3点」）は、高リスク群と低リスク群を分類する際の参考閾値となります。この閾値を用いると、感度0.438（SPK発症者の約44%を高リスクと判定）、特異度0.771（SPK非発症者の約77%を低リスクと判定）となりました。実際の運用では、感度と特異度のバランスを考慮してカットオフ値を決定する必要があります。\n",
        "\n",
        "## 5. 限界と注意点\n",
        "\n",
        "*   **データセット依存性 (外的妥当性):** このスコアは特定のデータセットに基づいて開発されたため、異なる患者集団（人種、年齢構成、術式など）に適用した場合、同じ性能が得られるとは限りません。\n",
        "*   **他の因子の影響:** スコアに含まれていない他の因子（年齢、性別、涙液量、基礎疾患、薬剤使用など）や、術中・術後の要因もSPKリスクに影響を与える可能性があります。\n",
        "*   **臨床判断の補助:** 本スコアはリスク評価ツールであり、確定診断を行うものではありません。必ず他の臨床所見と合わせて総合的に判断する必要があります。\n",
        "*   **ΔMRD-1の定義の問題:** 前述の通り、ΔMRD-1に術後データが含まれている点に注意が必要です。\n",
        "\n",
        "## 6. まとめ\n",
        "\n",
        "この術後SPKリスクスコアリングシステムは、統計解析に基づき特定された因子を組み合わせて、患者個々のリスクレベルを評価する試みです。客観的な指標によるリスク層別化を可能にし、ハイリスク患者への重点的な管理計画立案に貢献する可能性があります。ただし、その限界（特にΔMRD-1の扱いと外的妥当性）を理解し、臨床判断を補助するツールとして慎重に活用することが求められます。"
      ],
      "metadata": {
        "id": "3O0mcK0yWReC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 術後因子を加えたSPKリスクスコアリングシステム：設計方針と構成案\n",
        "\n",
        "## 設計方針\n",
        "\n",
        "既存の術前因子ベースのスコア（AUC 0.659）の性能（AUC 0.710）を改善するため、統計解析結果に基づき術後因子を追加した新しいスコアリングシステムを設計します。\n",
        "\n",
        "1.  **ベースの維持:**\n",
        "    *   既存スコアの構成要素（MRD-1 pre, ΔMRD-1, BUT pre, levator function pre）は、単変量解析またはForest Plotで術後SPKリスクとの関連が示唆されているため、基本的には維持します。\n",
        "    *   既存の点数配分（MRD-1 pre < 0.5mm に2点、他は1点）も踏襲します。\n",
        "\n",
        "2.  **追加因子の選定:**\n",
        "    *   **BUT post** を追加します。この因子は以下の理由で選択されました。\n",
        "        *   多変量解析（フルモデル、AIC選択モデル）で、術後SPKとの独立した強い関連性が一貫して示されました (AICモデル OR ≈ 0.69, p=0.002)。\n",
        "        *   Forest Plot分析でも、BUT postが低い四分位（Q1: <3秒, Q2: 3-4秒）でオッズ比が有意に高いことが確認されました。\n",
        "    *   **カットオフ値:** Forest Plotの結果から、BUT post `< 4秒` をリスクの閾値として設定します。\n",
        "    *   **点数配分:** AICモデルにおけるBUT postのオッズ比（約0.69）が、既存スコアに含まれるlevator function pre（OR約0.63）と同程度のリスク寄与を示唆することを考慮し、`BUT post < 4秒` には**1点**を割り当てます。\n",
        "\n",
        "3.  **含めない因子:**\n",
        "    *   `MRD-2 3M` はAIC選択モデルには含まれましたが、単変量解析やForest Plotでの関連性が不明瞭であり、かつORも他の有意な因子ほど顕著ではないため、スコアの簡便性と解釈の容易さを優先し、今回は**含めません**。\n",
        "\n",
        "4.  **ΔMRD-1の扱い:**\n",
        "    *   `ΔMRD-1` は術後3ヶ月のデータを含むため、「純粋な術前スコア」ではないという注意点はありますが、既存スコアとの比較を行うため、今回の新しいスコアにも**そのまま含めます**。\n",
        "\n",
        "## 新しいスコアリングシステムの構成（案）\n",
        "\n",
        "上記の方針に基づき、以下の構成で新しいリスクスコア（合計0点〜6点）を定義します。\n",
        "\n",
        "| リスク因子               | 条件（カットオフ値） | 割り当て点数 |\n",
        "| :----------------------- | :----------------- | :--------- |\n",
        "| MRD-1 pre                | `< 0.5 mm`         | 2点        |\n",
        "| ΔMRD-1                   | `>= 2.0 mm`        | 1点        |\n",
        "| BUT pre                  | `< 5 秒`           | 1点        |\n",
        "| levator function pre     | `< 8 mm`           | 1点        |\n",
        "| **BUT post**             | **`< 4 秒`**       | **1点**    |\n",
        "| **合計スコア**           |                    | **0点 〜 6点** |"
      ],
      "metadata": {
        "id": "dQOox8BGWVzB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6BdBW1AYWSEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic regressionモデル作成**\n"
      ],
      "metadata": {
        "id": "8xlcb3OoWab-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：Top N AUC内 最少変数モデル自動選択 + CV\n",
        "# =============================================================\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. ライブラリのインポートと基本設定 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "# (以下、他の警告抑制も同様)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "\n",
        "print(\"Libraries imported and warnings configured.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Google Drive マウント と ファイル/フォルダ設定 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) # 必要に応じて再マウント\n",
        "    file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    output_dir = \"/content/best_subset_parsimony_results\" # 新しいフォルダ名\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Running in a non-Colab environment.\")\n",
        "    file_path = \"眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    output_dir = \"./best_subset_parsimony_results\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. データ読み込みと前処理 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 2. Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "    print(f\"Excel file loaded successfully. Original shape: {df_all.shape}\")\n",
        "except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); exit()\n",
        "except Exception as e: print(f\"Error loading Excel file: {e}\"); exit()\n",
        "\n",
        "cols_to_convert = [\"MRD-1 pre\", \"MRD-1 3M\", \"MRD-2 pre\", \"MRD-2 3M\", \"levator_function pre\", \"BUT pre\", \"BUT post\", \"SPK pre\", \"SPK post\"]\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns: df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "    else: print(f\"Warning: Column '{col}' not found.\")\n",
        "\n",
        "if \"MRD-1 3M\" in df_all and \"MRD-1 pre\" in df_all: df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "else: df_all[\"ΔMRD-1\"] = np.nan\n",
        "if \"MRD-2 3M\" in df_all and \"MRD-2 pre\" in df_all: df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "else: df_all[\"ΔMRD-2\"] = np.nan\n",
        "\n",
        "if \"SPK pre\" in df_all: df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "else: print(\"Error: 'SPK pre' column not found.\"); exit()\n",
        "if \"SPK post\" not in df: print(\"Error: 'SPK post' column not found.\"); exit()\n",
        "\n",
        "df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "target_col = 'SPK post'\n",
        "\n",
        "print(f\"Analysis data shape (SPK pre=0, SPK post=0 or 1): {df_analysis.shape}\")\n",
        "if df_analysis.empty or df_analysis[target_col].nunique() < 2: print(\"Error: Insufficient data.\"); exit()\n",
        "print(f\"Target variable '{target_col}' distribution:\\n{df_analysis[target_col].value_counts()}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. 総当たり実行関数 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "# run_best_subset_logit 関数は前回のままでOK (VIF計算は含まれていても問題ない)\n",
        "def run_best_subset_logit(df, target_col, candidate_cols):\n",
        "    # (前回のコードと同じ内容 - ここでは省略)\n",
        "    results = []\n",
        "    n_candidates = len(candidate_cols)\n",
        "    valid_candidate_cols = [col for col in candidate_cols if col in df.columns]\n",
        "    n_candidates = len(valid_candidate_cols)\n",
        "    if n_candidates == 0: return pd.DataFrame()\n",
        "\n",
        "    total_combinations = sum(1 for k in range(1, n_candidates + 1) for _ in combinations(valid_candidate_cols, k))\n",
        "    print(f\"Total combinations to evaluate: {total_combinations} using {n_candidates} candidate variables.\")\n",
        "\n",
        "    # --- VIF計算ヘルパー関数を内部で定義 ---\n",
        "    def calculate_max_vif_internal(df_sub, predictor_cols_sub):\n",
        "        if not predictor_cols_sub or len(predictor_cols_sub) < 2: return 0.0\n",
        "        X_vif_sub = df_sub[predictor_cols_sub].copy() # df_sub は既にdropna済みと仮定\n",
        "        if X_vif_sub.shape[0] < 2: return np.inf\n",
        "        try:\n",
        "            vif_values_sub = [variance_inflation_factor(X_vif_sub.values, i) for i in range(X_vif_sub.shape[1])]\n",
        "            max_vif_val = np.max(vif_values_sub)\n",
        "            return np.inf if np.isinf(max_vif_val) else max_vif_val\n",
        "        except Exception: return np.inf\n",
        "    # --- ここまで VIF計算ヘルパー ---\n",
        "\n",
        "    for k in tqdm(range(1, n_candidates + 1), desc=\"Evaluating subsets\"):\n",
        "        for subset_cols in combinations(valid_candidate_cols, k):\n",
        "            subset_cols_list = list(subset_cols)\n",
        "            required_cols = [target_col] + subset_cols_list\n",
        "            df_subset = df[required_cols].dropna()\n",
        "            n_samples = len(df_subset)\n",
        "            n_classes = df_subset[target_col].nunique()\n",
        "            min_samples_needed = len(subset_cols_list) + 2\n",
        "\n",
        "            if n_samples < min_samples_needed or n_classes < 2:\n",
        "                results.append({'num_vars': k, 'variables': ', '.join(subset_cols_list), 'n_samples': n_samples, 'auc': np.nan, 'model': None, 'error': 'Insufficient data', 'max_vif': np.nan})\n",
        "                continue\n",
        "\n",
        "            y = df_subset[target_col]\n",
        "            X = df_subset[subset_cols_list]\n",
        "            X_const = sm.add_constant(X, has_constant='add')\n",
        "\n",
        "            auc = np.nan; model_fit = None; error_msg = None; max_vif = np.nan\n",
        "\n",
        "            try:\n",
        "                logit_model = sm.Logit(y, X_const)\n",
        "                model_fit = logit_model.fit(disp=False, maxiter=100, warn_convergence=False)\n",
        "\n",
        "                if model_fit.mle_retvals['converged']:\n",
        "                    y_pred_prob = model_fit.predict(X_const)\n",
        "                    auc = roc_auc_score(y, y_pred_prob)\n",
        "                    max_vif = calculate_max_vif_internal(df_subset, subset_cols_list) # VIFも計算しておく\n",
        "                else: error_msg = 'Convergence Failed'\n",
        "\n",
        "            except PerfectSeparationError: error_msg = 'PerfectSeparationError'; max_vif = np.inf\n",
        "            except np.linalg.LinAlgError: error_msg = 'LinAlgError'; max_vif = np.inf\n",
        "            except ValueError as ve: error_msg = f'ValueError: {ve}'; max_vif = np.inf\n",
        "            except Exception as e: error_msg = f'Other Error: {type(e).__name__}'; max_vif = np.inf\n",
        "\n",
        "            results.append({'num_vars': k, 'variables': ', '.join(subset_cols_list), 'n_samples': n_samples, 'auc': auc, 'model': model_fit, 'error': error_msg, 'max_vif': max_vif})\n",
        "\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df = results_df.sort_values(by='auc', ascending=False, na_position='last').reset_index(drop=True)\n",
        "    return results_df\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. 共線性チェック関数 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "# check_collinearity 関数は前回のコードと同じものを使用します\n",
        "def check_collinearity(df, predictor_cols, vif_threshold=5.0, output_dir=\".\"):\n",
        "    # (前回のコードと同じ内容 - ここでは省略)\n",
        "    if not predictor_cols or len(predictor_cols) < 2: print(\"Collinearity check skipped.\"); return None, None\n",
        "    print(f\"\\n--- Detailed Collinearity Check for variables: {', '.join(predictor_cols)} ---\")\n",
        "    df_check = df[predictor_cols].dropna();\n",
        "    if df_check.shape[0] < 2: print(\"Warning: Insufficient data.\"); return None, None\n",
        "    corr_matrix=None; vif_data=None # Initialize\n",
        "    try: # Correlation\n",
        "        corr_matrix = df_check.corr(); plt.figure(figsize=(min(10, len(predictor_cols)+2), min(8, len(predictor_cols)+1)))\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
        "        plt.title(\"Correlation Matrix\"); plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0); plt.tight_layout()\n",
        "        corr_fig_path = os.path.join(output_dir, f\"corr_matrix_{'_'.join(predictor_cols[:3])}.png\")\n",
        "        plt.savefig(corr_fig_path, dpi=300); print(f\"Correlation matrix heatmap saved to {corr_fig_path}\"); plt.show()\n",
        "        print(\"\\nCorrelation Matrix:\"); display(corr_matrix.round(3))\n",
        "        high_corr_pairs = corr_matrix.unstack().sort_values(ascending=False).drop_duplicates(); high_corr_pairs = high_corr_pairs[high_corr_pairs < 1.0]\n",
        "        significant_high_corr = high_corr_pairs[abs(high_corr_pairs) > 0.7]; print(\"\\nHighly correlated pairs (abs > 0.7):\")\n",
        "        if not significant_high_corr.empty:\n",
        "            display(significant_high_corr)\n",
        "        else:\n",
        "            print(\"None found.\")\n",
        "    except Exception as e: print(f\"Error during correlation analysis: {e}\")\n",
        "    if len(predictor_cols) >= 2: # VIF\n",
        "        X_vif = df_check.copy()\n",
        "        try:\n",
        "            vif_data = pd.DataFrame(); vif_data[\"Variable\"] = X_vif.columns; vif_values = []\n",
        "            for i in range(X_vif.shape[1]):\n",
        "                 try: v = variance_inflation_factor(X_vif.values, i); vif_values.append(v)\n",
        "                 except Exception as vif_e: print(f\"VIF Error for {X_vif.columns[i]}: {vif_e}\"); vif_values.append(np.nan)\n",
        "            vif_data[\"VIF\"] = vif_values; print(\"\\nVariance Inflation Factor (VIF):\"); display(vif_data.round(3))\n",
        "            high_vif_vars = vif_data[vif_data[\"VIF\"] > vif_threshold]\n",
        "            if not high_vif_vars.empty: print(f\"\\nWarning: Variables with VIF > {vif_threshold}:\"); display(high_vif_vars)\n",
        "            else: print(f\"\\nNo variables found with VIF > {vif_threshold}.\")\n",
        "        except Exception as e: print(f\"\\nCould not calculate VIF: {e}\")\n",
        "    return corr_matrix, vif_data\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. メイン処理: 自動選択(TopN内最少変数)と評価\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 5. Main Processing: Automatic Selection (Parsimony in Top N AUC) & Evaluation ---\")\n",
        "\n",
        "# パラメータ設定\n",
        "TOP_N_MODELS = 10 # 上位何件のAUCモデルを考慮するか\n",
        "VIF_THRESHOLD_FOR_CHECK = 5.0 # 最終チェック用のVIF閾値 (選択基準ではない)\n",
        "print(f\"Selection Criteria: Most parsimonious model within Top {TOP_N_MODELS} Training AUC scores.\")\n",
        "\n",
        "# --- 5a. 術後モデル ---\n",
        "print(\"\\n======================================================\")\n",
        "print(\"=== Post-operative Model: Auto Selection & Evaluation ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "post_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT post\",\n",
        "                       \"MRD-2 pre\", \"ΔMRD-2\", \"MRD-2 3M\", \"BUT pre\"]\n",
        "post_candidate_cols_valid = [col for col in post_candidate_cols if col in df_analysis.columns]\n",
        "\n",
        "selected_post_model = None\n",
        "selected_post_vars = []\n",
        "mean_cv_auc_post = np.nan\n",
        "post_results_df = pd.DataFrame() # 結果格納用\n",
        "\n",
        "if post_candidate_cols_valid:\n",
        "    start_time_post = time.time()\n",
        "    post_results_df = run_best_subset_logit(df_analysis, target_col, post_candidate_cols_valid)\n",
        "    end_time_post = time.time()\n",
        "    print(f\"\\nBest subset search took {end_time_post - start_time_post:.2f} seconds.\")\n",
        "\n",
        "    # 有効なモデル（エラーなし、AUC計算済み）をフィルタリング\n",
        "    valid_models_post = post_results_df[\n",
        "        post_results_df['error'].isna() & post_results_df['auc'].notna()\n",
        "    ].copy()\n",
        "\n",
        "    if not valid_models_post.empty:\n",
        "        # 上位 N 件を取得\n",
        "        top_n_post = valid_models_post.head(TOP_N_MODELS)\n",
        "        print(f\"\\n--- Top {TOP_N_MODELS} Models (Post-operative based on Training AUC) ---\")\n",
        "        display(top_n_post[['num_vars', 'variables', 'n_samples', 'auc', 'max_vif']].round({'auc': 4, 'max_vif': 3})) # VIFも参考表示\n",
        "\n",
        "        if not top_n_post.empty:\n",
        "            # Top N の中で最小の変数数を見つける\n",
        "            min_vars_in_top_n = top_n_post['num_vars'].min()\n",
        "\n",
        "            # 最小変数数のモデルを抽出\n",
        "            most_parsimonious_top_n = top_n_post[top_n_post['num_vars'] == min_vars_in_top_n]\n",
        "\n",
        "            # その中でAUCが最高のモデルを選択 (既にソートされているので先頭)\n",
        "            auto_selected_post_row = most_parsimonious_top_n.iloc[0]\n",
        "\n",
        "            selected_post_vars = auto_selected_post_row['variables'].split(', ')\n",
        "            selected_post_model = auto_selected_post_row['model']\n",
        "\n",
        "            print(f\"\\n--- Best Post-operative Model (Auto Selected: Parsimony in Top {TOP_N_MODELS} AUC) ---\")\n",
        "            print(f\"Variables ({auto_selected_post_row['num_vars']}): {', '.join(selected_post_vars)}\")\n",
        "            print(f\"Training AUC: {auto_selected_post_row['auc']:.4f}\")\n",
        "            print(f\"(Model ranked #{auto_selected_post_row.name + 1} overall by AUC)\") # 元のDFでの順位\n",
        "            print(f\"Number of samples used: {auto_selected_post_row['n_samples']}\")\n",
        "            print(f\"Max VIF (for info): {auto_selected_post_row['max_vif']:.3f}\") # 参考情報\n",
        "\n",
        "            # 詳細な共線性チェックとサマリー表示\n",
        "            if selected_post_model:\n",
        "                print(\"\\n--- Detailed Collinearity Check (Auto Selected Post-op Model) ---\")\n",
        "                _ = check_collinearity(df_analysis, selected_post_vars, vif_threshold=VIF_THRESHOLD_FOR_CHECK, output_dir=output_dir)\n",
        "                print(f\"\\n--- Summary (Auto Selected Post-op Model) ---\")\n",
        "                try:\n",
        "                    xnames_post_auto = ['Intercept'] + selected_post_vars\n",
        "                    print(selected_post_model.summary(xname=xnames_post_auto))\n",
        "                    print(\"\\nOdds Ratios:\")\n",
        "                    conf_auto_post = selected_post_model.conf_int()\n",
        "                    conf_auto_post['Odds Ratio'] = selected_post_model.params\n",
        "                    conf_auto_post.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                    display(np.exp(conf_auto_post).round(3))\n",
        "                except Exception as e: print(f\"Could not display model summary: {e}\")\n",
        "            else: print(\"Model object not available for summary.\")\n",
        "        else:\n",
        "            print(f\"\\nWarning: No valid models found within the Top {TOP_N_MODELS} AUC ranks.\")\n",
        "    else:\n",
        "        print(\"\\nWarning: No valid models found from the best subset search.\")\n",
        "else:\n",
        "    print(\"\\nSkipping post-operative model search: No valid candidate variables.\")\n",
        "\n",
        "\n",
        "# --- 5b. 術前モデル ---\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(\"=== Pre-operative Model: Auto Selection & Evaluation ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "preop_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\",\n",
        "                        \"MRD-2 pre\", \"BUT pre\"]\n",
        "preop_candidate_cols_valid = [col for col in preop_candidate_cols if col in df_analysis.columns]\n",
        "\n",
        "selected_preop_model = None\n",
        "selected_preop_vars = []\n",
        "mean_cv_auc_preop = np.nan\n",
        "preop_results_df = pd.DataFrame() # 結果格納用\n",
        "\n",
        "if preop_candidate_cols_valid:\n",
        "    start_time_preop = time.time()\n",
        "    preop_results_df = run_best_subset_logit(df_analysis, target_col, preop_candidate_cols_valid)\n",
        "    end_time_preop = time.time()\n",
        "    print(f\"\\nBest subset search took {end_time_preop - start_time_preop:.2f} seconds.\")\n",
        "\n",
        "    # 有効なモデルをフィルタリング\n",
        "    valid_models_preop = preop_results_df[\n",
        "        preop_results_df['error'].isna() & preop_results_df['auc'].notna()\n",
        "    ].copy()\n",
        "\n",
        "    if not valid_models_preop.empty:\n",
        "        # 上位 N 件を取得\n",
        "        top_n_preop = valid_models_preop.head(TOP_N_MODELS)\n",
        "        print(f\"\\n--- Top {TOP_N_MODELS} Models (Pre-operative based on Training AUC) ---\")\n",
        "        display(top_n_preop[['num_vars', 'variables', 'n_samples', 'auc', 'max_vif']].round({'auc': 4, 'max_vif': 3}))\n",
        "\n",
        "        if not top_n_preop.empty:\n",
        "            # Top N 内で最小変数数\n",
        "            min_vars_in_top_n_preop = top_n_preop['num_vars'].min()\n",
        "            # 最小変数数のモデルを抽出\n",
        "            most_parsimonious_top_n_preop = top_n_preop[top_n_preop['num_vars'] == min_vars_in_top_n_preop]\n",
        "            # AUC最高のものを選択\n",
        "            auto_selected_preop_row = most_parsimonious_top_n_preop.iloc[0]\n",
        "\n",
        "            selected_preop_vars = auto_selected_preop_row['variables'].split(', ')\n",
        "            selected_preop_model = auto_selected_preop_row['model']\n",
        "\n",
        "            print(f\"\\n--- Best Pre-operative Model (Auto Selected: Parsimony in Top {TOP_N_MODELS} AUC) ---\")\n",
        "            print(f\"Variables ({auto_selected_preop_row['num_vars']}): {', '.join(selected_preop_vars)}\")\n",
        "            print(f\"Training AUC: {auto_selected_preop_row['auc']:.4f}\")\n",
        "            print(f\"(Model ranked #{auto_selected_preop_row.name + 1} overall by AUC)\")\n",
        "            print(f\"Number of samples used: {auto_selected_preop_row['n_samples']}\")\n",
        "            print(f\"Max VIF (for info): {auto_selected_preop_row['max_vif']:.3f}\")\n",
        "\n",
        "            # 詳細チェックとサマリー\n",
        "            if selected_preop_model:\n",
        "                print(\"\\n--- Detailed Collinearity Check (Auto Selected Pre-op Model) ---\")\n",
        "                _ = check_collinearity(df_analysis, selected_preop_vars, vif_threshold=VIF_THRESHOLD_FOR_CHECK, output_dir=output_dir)\n",
        "                print(f\"\\n--- Summary (Auto Selected Pre-op Model) ---\")\n",
        "                try:\n",
        "                    xnames_preop_auto = ['Intercept'] + selected_preop_vars\n",
        "                    print(selected_preop_model.summary(xname=xnames_preop_auto))\n",
        "                    print(\"\\nOdds Ratios:\")\n",
        "                    conf_auto_preop = selected_preop_model.conf_int()\n",
        "                    conf_auto_preop['Odds Ratio'] = selected_preop_model.params\n",
        "                    conf_auto_preop.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                    display(np.exp(conf_auto_preop).round(3))\n",
        "                except Exception as e: print(f\"Could not display model summary: {e}\")\n",
        "            else: print(\"Model object not available for summary.\")\n",
        "        else:\n",
        "            print(f\"\\nWarning: No valid models found within the Top {TOP_N_MODELS} AUC ranks.\")\n",
        "    else:\n",
        "        print(\"\\nWarning: No valid models found from the best subset search.\")\n",
        "else:\n",
        "    print(\"\\nSkipping pre-operative model search: No valid candidate variables.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. クロスバリデーション (自動選択されたモデルに対して)\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n--- 6. Performing 5-Fold Cross-Validation on Auto-Selected Models ---\")\n",
        "n_splits = 5\n",
        "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# --- CV用 術後モデル ---\n",
        "if selected_post_vars:\n",
        "    print(f\"\\n--- Cross-Validation for Auto-Selected Post-operative Model ({len(selected_post_vars)} vars) ---\")\n",
        "    df_cv_post = df_analysis[[target_col] + selected_post_vars].dropna()\n",
        "    y_cv_post = df_cv_post[target_col]\n",
        "    X_cv_post = df_cv_post[selected_post_vars]\n",
        "\n",
        "    if X_cv_post.shape[0] >= n_splits and y_cv_post.nunique() == 2:\n",
        "        pipeline_post = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('logistic', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))\n",
        "        ])\n",
        "        try:\n",
        "            cv_auc_scores_post = cross_val_score(pipeline_post, X_cv_post, y_cv_post, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "            mean_cv_auc_post = cv_auc_scores_post.mean()\n",
        "            std_cv_auc_post = cv_auc_scores_post.std()\n",
        "            print(f\"Cross-Validation AUC Scores: {np.round(cv_auc_scores_post, 4)}\")\n",
        "            print(f\"Mean CV AUC: {mean_cv_auc_post:.4f}\")\n",
        "            print(f\"Std Dev CV AUC: {std_cv_auc_post:.4f}\")\n",
        "        except Exception as e: print(f\"Cross-validation failed: {e}\")\n",
        "    else: print(\"Insufficient data/classes for CV.\")\n",
        "else: print(\"\\nSkipping CV for Post-operative model: No model was automatically selected.\")\n",
        "\n",
        "# --- CV用 術前モデル ---\n",
        "if selected_preop_vars:\n",
        "    print(f\"\\n--- Cross-Validation for Auto-Selected Pre-operative Model ({len(selected_preop_vars)} vars) ---\")\n",
        "    df_cv_preop = df_analysis[[target_col] + selected_preop_vars].dropna()\n",
        "    y_cv_preop = df_cv_preop[target_col]\n",
        "    X_cv_preop = df_cv_preop[selected_preop_vars]\n",
        "\n",
        "    if X_cv_preop.shape[0] >= n_splits and y_cv_preop.nunique() == 2:\n",
        "        pipeline_preop = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('logistic', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))\n",
        "        ])\n",
        "        try:\n",
        "            cv_auc_scores_preop = cross_val_score(pipeline_preop, X_cv_preop, y_cv_preop, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
        "            mean_cv_auc_preop = cv_auc_scores_preop.mean()\n",
        "            std_cv_auc_preop = cv_auc_scores_preop.std()\n",
        "            print(f\"Cross-Validation AUC Scores: {np.round(cv_auc_scores_preop, 4)}\")\n",
        "            print(f\"Mean CV AUC: {mean_cv_auc_preop:.4f}\")\n",
        "            print(f\"Std Dev CV AUC: {std_cv_auc_preop:.4f}\")\n",
        "        except Exception as e: print(f\"Cross-validation failed: {e}\")\n",
        "    else: print(\"Insufficient data/classes for CV.\")\n",
        "else: print(\"\\nSkipping CV for Pre-operative model: No model was automatically selected.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 7. 最終結果サマリー\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(\"=== Final Auto-Selected Model Summary (Parsimony in Top N AUC) ===\")\n",
        "print(\"======================================================\")\n",
        "print(f\"Selection based on: Most parsimonious model within Top {TOP_N_MODELS} Training AUC scores.\")\n",
        "print(f\"VIF Threshold used for final check (not selection): {VIF_THRESHOLD_FOR_CHECK}\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Post-operative Model ---\")\n",
        "if selected_post_vars:\n",
        "    # 再度ベスト行を取得して表示\n",
        "    auto_selected_post_row = top_n_post[top_n_post['num_vars'] == top_n_post['num_vars'].min()].iloc[0]\n",
        "    print(f\"Selected Variables ({auto_selected_post_row['num_vars']}): {', '.join(selected_post_vars)}\")\n",
        "    print(f\"Training AUC: {auto_selected_post_row['auc']:.4f} (Ranked #{auto_selected_post_row.name + 1} overall)\")\n",
        "    print(f\"Max VIF (for info): {auto_selected_post_row['max_vif']:.3f}\")\n",
        "    print(f\"Mean 5-Fold CV AUC: {mean_cv_auc_post:.4f}\")\n",
        "else:\n",
        "    print(\"No suitable model selected based on the criteria.\")\n",
        "\n",
        "print(\"\\n--- Pre-operative Model ---\")\n",
        "if selected_preop_vars:\n",
        "    # 再度ベスト行を取得して表示\n",
        "    auto_selected_preop_row = top_n_preop[top_n_preop['num_vars'] == top_n_preop['num_vars'].min()].iloc[0]\n",
        "    print(f\"Selected Variables ({auto_selected_preop_row['num_vars']}): {', '.join(selected_preop_vars)}\")\n",
        "    print(f\"Training AUC: {auto_selected_preop_row['auc']:.4f} (Ranked #{auto_selected_preop_row.name + 1} overall)\")\n",
        "    print(f\"Max VIF (for info): {auto_selected_preop_row['max_vif']:.3f}\")\n",
        "    print(f\"Mean 5-Fold CV AUC: {mean_cv_auc_preop:.4f}\")\n",
        "else:\n",
        "    print(\"No suitable model selected based on the criteria.\")\n",
        "\n",
        "print(\"\\nReminder: High Training AUC with a significant drop in CV AUC suggests overfitting.\")\n",
        "print(\"Review the selected model's summary, VIF check, and CV results to make a final decision.\")\n",
        "print(f\"\\nAnalysis Complete. Check '{output_dir}' for correlation heatmaps.\")"
      ],
      "metadata": {
        "id": "I5cmiZGZWbD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：必須変数指定 + Top10 CVによるベストモデル選択\n",
        "# =============================================================\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. ライブラリのインポートと基本設定\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from tqdm.auto import tqdm\n",
        "import joblib # CV結果の保存/読み込み用 (オプション)\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "\n",
        "print(\"Libraries imported and warnings configured.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Google Drive マウント と ファイル/フォルダ設定\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True) # 必要に応じて再マウント\n",
        "    # ★★★ Google Drive内のExcelファイルのパスを指定してください ★★★\n",
        "    file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    # ★★★ 結果を出力するフォルダを指定してください ★★★\n",
        "    output_dir = \"/content/best_subset_required_cv_results\" # 新しいフォルダ名\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Running in a non-Colab environment.\")\n",
        "    # ★★★ ローカルのExcelファイルのパスを指定してください ★★★\n",
        "    file_path = \"眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    # ★★★ 結果を出力するフォルダを指定してください ★★★\n",
        "    output_dir = \"./best_subset_required_cv_results\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. データ読み込みと前処理\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 2. Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "    print(f\"Excel file loaded successfully. Original shape: {df_all.shape}\")\n",
        "except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); exit()\n",
        "except Exception as e: print(f\"Error loading Excel file: {e}\"); exit()\n",
        "\n",
        "# --- 数値変換、ΔMRD計算、フィルタリング (変更なし) ---\n",
        "cols_to_convert = [\"MRD-1 pre\", \"MRD-1 3M\", \"MRD-2 pre\", \"MRD-2 3M\", \"levator_function pre\", \"BUT pre\", \"BUT post\", \"SPK pre\", \"SPK post\"]\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns: df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "    else: print(f\"Warning: Column '{col}' not found.\")\n",
        "\n",
        "if \"MRD-1 3M\" in df_all and \"MRD-1 pre\" in df_all: df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "else: df_all[\"ΔMRD-1\"] = np.nan\n",
        "if \"MRD-2 3M\" in df_all and \"MRD-2 pre\" in df_all: df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "else: df_all[\"ΔMRD-2\"] = np.nan\n",
        "\n",
        "if \"SPK pre\" in df_all: df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "else: print(\"Error: 'SPK pre' column not found.\"); exit()\n",
        "if \"SPK post\" not in df: print(\"Error: 'SPK post' column not found.\"); exit()\n",
        "\n",
        "df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "target_col = 'SPK post'\n",
        "\n",
        "print(f\"Analysis data shape (SPK pre=0, SPK post=0 or 1): {df_analysis.shape}\")\n",
        "if df_analysis.empty or df_analysis[target_col].nunique() < 2: print(\"Error: Insufficient data.\"); exit()\n",
        "print(f\"Target variable '{target_col}' distribution:\\n{df_analysis[target_col].value_counts()}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. 総当たり実行関数 (必須変数指定に対応) ★★★ 修正箇所 ★★★\n",
        "# -------------------------------------------------------------\n",
        "def run_best_subset_logit_required(df, target_col, candidate_cols, required_cols=[]):\n",
        "    \"\"\"\n",
        "    必須変数を指定可能なロジスティック回帰の総当たり変数選択を実行し、AUCを計算する関数。\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # --- 入力チェック ---\n",
        "    valid_candidate_cols = [col for col in candidate_cols if col in df.columns]\n",
        "    valid_required_cols = [col for col in required_cols if col in df.columns]\n",
        "\n",
        "    missing_required = set(required_cols) - set(valid_required_cols)\n",
        "    if missing_required:\n",
        "        print(f\"Warning: Required columns not found in dataframe and ignored: {list(missing_required)}\")\n",
        "\n",
        "    missing_in_candidates = set(valid_required_cols) - set(valid_candidate_cols)\n",
        "    if missing_in_candidates:\n",
        "         print(f\"Warning: Required columns are not in candidate_cols but will be included: {list(missing_in_candidates)}\")\n",
        "         valid_candidate_cols = list(set(valid_candidate_cols) | set(valid_required_cols))\n",
        "\n",
        "    optional_cols = [col for col in valid_candidate_cols if col not in valid_required_cols]\n",
        "    n_optional = len(optional_cols)\n",
        "    n_required = len(valid_required_cols)\n",
        "\n",
        "    if n_required == 0 and n_optional == 0:\n",
        "        print(\"Error: No valid candidate or required columns found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    total_combinations = sum(1 for k in range(n_optional + 1) for _ in combinations(optional_cols, k))\n",
        "    print(f\"Total combinations to evaluate: {total_combinations} (required: {n_required}, optional: {n_optional})\")\n",
        "\n",
        "    # --- VIF計算ヘルパー関数 ---\n",
        "    def calculate_max_vif_internal(df_sub, predictor_cols_sub):\n",
        "        if not predictor_cols_sub or len(predictor_cols_sub) < 2: return 0.0\n",
        "        X_vif_sub = df_sub[predictor_cols_sub].copy()\n",
        "        if X_vif_sub.shape[0] < 2: return np.inf\n",
        "        try:\n",
        "            is_constant = X_vif_sub.std() < 1e-9\n",
        "            if is_constant.any(): X_vif_sub = X_vif_sub.loc[:, ~is_constant]\n",
        "            if X_vif_sub.shape[1] < 2 or X_vif_sub.empty: return np.inf\n",
        "            # VIF calculation can still fail if perfect collinearity exists after removing constant columns\n",
        "            try:\n",
        "                vif_values_sub = [variance_inflation_factor(X_vif_sub.values, i) for i in range(X_vif_sub.shape[1])]\n",
        "                max_vif_val = np.max(vif_values_sub); return np.inf if np.isinf(max_vif_val) or np.isnan(max_vif_val) else max_vif_val\n",
        "            except np.linalg.LinAlgError: # Catch explicit LinAlgError for perfect collinearity\n",
        "                 return np.inf\n",
        "        except Exception: return np.inf\n",
        "\n",
        "    # --- 組み合わせループ (オプション変数の数でループ) ---\n",
        "    for k in tqdm(range(n_optional + 1), desc=\"Evaluating subsets\"):\n",
        "        for optional_subset in combinations(optional_cols, k):\n",
        "            subset_cols_list = sorted(list(valid_required_cols) + list(optional_subset))\n",
        "            num_total_vars = len(subset_cols_list)\n",
        "            if num_total_vars == 0 : continue\n",
        "\n",
        "            required_cols_in_model = [target_col] + subset_cols_list\n",
        "            df_subset = df[required_cols_in_model].dropna()\n",
        "            n_samples = len(df_subset)\n",
        "            n_classes = df_subset[target_col].nunique()\n",
        "            min_samples_needed = num_total_vars + 2\n",
        "\n",
        "            if n_samples < min_samples_needed or n_classes < 2:\n",
        "                results.append({'num_vars': num_total_vars, 'variables': ', '.join(subset_cols_list), 'n_samples': n_samples, 'auc': np.nan, 'model': None, 'error': 'Insufficient data', 'max_vif': np.nan})\n",
        "                continue\n",
        "\n",
        "            y = df_subset[target_col]\n",
        "            X = df_subset[subset_cols_list]\n",
        "            X_const = sm.add_constant(X, has_constant='add')\n",
        "\n",
        "            auc = np.nan; model_fit = None; error_msg = None; max_vif = np.nan\n",
        "\n",
        "            try:\n",
        "                logit_model = sm.Logit(y, X_const)\n",
        "                model_fit = logit_model.fit(disp=False, maxiter=100, warn_convergence=False)\n",
        "                if model_fit.mle_retvals['converged']:\n",
        "                    y_pred_prob = model_fit.predict(X_const)\n",
        "                    if len(np.unique(np.round(y_pred_prob, 8))) > 1:\n",
        "                         auc = roc_auc_score(y, y_pred_prob); max_vif = calculate_max_vif_internal(df_subset, subset_cols_list)\n",
        "                    else: error_msg = 'Prediction is constant'; max_vif = np.inf\n",
        "                else: error_msg = 'Convergence Failed'\n",
        "            except PerfectSeparationError: error_msg = 'PerfectSeparationError'; max_vif = np.inf\n",
        "            except np.linalg.LinAlgError: error_msg = 'LinAlgError'; max_vif = np.inf\n",
        "            except ValueError as ve: error_msg = f'ValueError: {ve}'; max_vif = np.inf\n",
        "            except Exception as e: error_msg = f'Other Error: {type(e).__name__}'; max_vif = np.inf\n",
        "\n",
        "            results.append({'num_vars': num_total_vars, 'variables': ', '.join(subset_cols_list), 'n_samples': n_samples, 'auc': auc, 'model': model_fit, 'error': error_msg, 'max_vif': max_vif})\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values(by='auc', ascending=False, na_position='last').reset_index(drop=True)\n",
        "    return results_df\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. 共線性チェック関数 (変更なし)\n",
        "# -------------------------------------------------------------\n",
        "def check_collinearity(df, predictor_cols, vif_threshold=5.0, output_dir=\".\"):\n",
        "    if not predictor_cols or len(predictor_cols) < 2: print(\"Collinearity check skipped.\"); return None, None\n",
        "    print(f\"\\n--- Detailed Collinearity Check for variables: {', '.join(predictor_cols)} ---\")\n",
        "    df_check = df[predictor_cols].dropna();\n",
        "    if df_check.shape[0] < 2: print(\"Warning: Insufficient data.\"); return None, None\n",
        "    corr_matrix=None; vif_data=None # Initialize\n",
        "    try: # Correlation\n",
        "        corr_matrix = df_check.corr(); plt.figure(figsize=(min(10, len(predictor_cols)+2), min(8, len(predictor_cols)+1)))\n",
        "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, annot_kws={\"size\": 8})\n",
        "        plt.title(\"Correlation Matrix\"); plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0); plt.tight_layout()\n",
        "        filename_suffix = \"_\".join(predictor_cols).replace(' ','_').replace('<','').replace('>','').replace('.','').replace('-','').replace('Δ','d')[:50] # More robust filename\n",
        "        corr_fig_path = os.path.join(output_dir, f\"corr_matrix_{filename_suffix}.png\")\n",
        "        plt.savefig(corr_fig_path, dpi=300); print(f\"Correlation matrix heatmap saved to {corr_fig_path}\"); plt.show()\n",
        "        print(\"\\nCorrelation Matrix:\"); display(corr_matrix.round(3))\n",
        "        high_corr_pairs = corr_matrix.unstack().sort_values(ascending=False).drop_duplicates(); high_corr_pairs = high_corr_pairs[high_corr_pairs < 1.0]\n",
        "        significant_high_corr = high_corr_pairs[abs(high_corr_pairs) > 0.7]; print(\"\\nHighly correlated pairs (abs > 0.7):\")\n",
        "        if not significant_high_corr.empty: display(significant_high_corr)\n",
        "        else: print(\"None found.\")\n",
        "    except Exception as e: print(f\"Error during correlation analysis: {e}\")\n",
        "    if len(predictor_cols) >= 2: # VIF\n",
        "        X_vif = df_check.copy()\n",
        "        try:\n",
        "            is_constant = X_vif.std() < 1e-9\n",
        "            if is_constant.any():\n",
        "                 print(f\"Warning: Constant columns detected for VIF: {X_vif.columns[is_constant].tolist()}\")\n",
        "                 X_vif = X_vif.loc[:, ~is_constant]\n",
        "            if X_vif.shape[1] < 2 or X_vif.empty:\n",
        "                 print(\"Warning: Not enough non-constant variables for VIF calculation.\")\n",
        "                 return corr_matrix, None\n",
        "\n",
        "            vif_data = pd.DataFrame(); vif_data[\"Variable\"] = X_vif.columns; vif_values = []\n",
        "            for i in range(X_vif.shape[1]):\n",
        "                 try: v = variance_inflation_factor(X_vif.values, i); vif_values.append(v)\n",
        "                 except Exception as vif_e: print(f\"VIF Error for {X_vif.columns[i]}: {vif_e}\"); vif_values.append(np.nan)\n",
        "            vif_data[\"VIF\"] = vif_values; print(\"\\nVariance Inflation Factor (VIF):\"); display(vif_data.round(3))\n",
        "            high_vif_vars = vif_data[vif_data[\"VIF\"] > vif_threshold]\n",
        "            if not high_vif_vars.empty: print(f\"\\nWarning: Variables with VIF > {vif_threshold}:\"); display(high_vif_vars.round(3))\n",
        "            else: print(f\"\\nNo variables found with VIF > {vif_threshold}.\")\n",
        "        except Exception as e: print(f\"\\nCould not calculate VIF: {e}\")\n",
        "    return corr_matrix, vif_data\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. メイン処理: 必須変数指定 + Top10 CVによる選択\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 5. Main Processing: Required Vars + CV on Top N models ---\")\n",
        "\n",
        "# パラメータ設定\n",
        "TOP_N_MODELS = 100 # 上位何件の学習AUCモデルをCV評価するか\n",
        "N_SPLITS_CV = 10   # CVの分割数\n",
        "VIF_THRESHOLD_FINAL_CHECK = 5.0 # 最終モデルのVIFチェック閾値\n",
        "REQUIRED_VARIABLE = [\"BUT pre\"] # ★★★ 必ず含める変数のリスト ★★★\n",
        "print(f\"Selection Criteria: Best Mean {N_SPLITS_CV}-Fold CV AUC among Top {TOP_N_MODELS} Training AUC models (requiring specified variables).\")\n",
        "print(f\"Required variable(s): {', '.join(REQUIRED_VARIABLE)}\")\n",
        "\n",
        "# CV用パイプライン定義\n",
        "cv_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logistic', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))\n",
        "])\n",
        "cv_splitter = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=42)\n",
        "\n",
        "# --- 5a. 術後モデル ---\n",
        "print(\"\\n======================================================\")\n",
        "print(\"=== Post-operative Model: Required Vars + CV Selection ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "post_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT post\",\n",
        "                       \"MRD-2 pre\", \"ΔMRD-2\", \"MRD-2 3M\", \"BUT pre\"]\n",
        "# Ensure required variables are in the candidate list for the function\n",
        "post_candidate_cols_effective = list(set(post_candidate_cols) | set(REQUIRED_VARIABLE))\n",
        "print(f\"Post-operative candidate variables ({len(post_candidate_cols_effective)}): {', '.join(sorted(post_candidate_cols_effective))}\")\n",
        "\n",
        "\n",
        "cv_results_post = []\n",
        "best_cv_model_post_info = None\n",
        "post_results_df = pd.DataFrame() # Initialize\n",
        "\n",
        "# Check if all required variables exist in the dataframe\n",
        "if not all(req_var in df_analysis.columns for req_var in REQUIRED_VARIABLE):\n",
        "    print(f\"Error: Not all required variables {REQUIRED_VARIABLE} exist in the dataframe. Skipping post-operative analysis.\")\n",
        "else:\n",
        "    start_time_post_search = time.time()\n",
        "    # ★★★ 必須変数指定で関数呼び出し ★★★\n",
        "    post_results_df = run_best_subset_logit_required(df_analysis, target_col, post_candidate_cols_effective, required_cols=REQUIRED_VARIABLE)\n",
        "    end_time_post_search = time.time()\n",
        "    print(f\"\\nBest subset search took {end_time_post_search - start_time_post_search:.2f} seconds.\")\n",
        "\n",
        "    # 有効なモデルをフィルタリング\n",
        "    valid_models_post = post_results_df[post_results_df['error'].isna() & post_results_df['auc'].notna()].copy()\n",
        "\n",
        "    if not valid_models_post.empty:\n",
        "        # Top N を取得\n",
        "        top_n_post = valid_models_post.head(TOP_N_MODELS)\n",
        "        print(f\"\\n--- Performing {N_SPLITS_CV}-Fold CV on Top {len(top_n_post)} Training AUC Models (Post-operative, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "\n",
        "        start_time_post_cv = time.time()\n",
        "        for index, row in tqdm(top_n_post.iterrows(), total=len(top_n_post), desc=\"CV on Top Models\"):\n",
        "            current_vars = row['variables'].split(', ')\n",
        "            df_cv_post = df_analysis[[target_col] + current_vars].dropna()\n",
        "            y_cv_post = df_cv_post[target_col]\n",
        "            X_cv_post = df_cv_post[current_vars]\n",
        "            mean_auc = np.nan; std_auc = np.nan; cv_scores = []\n",
        "\n",
        "            can_cv = False\n",
        "            if X_cv_post.shape[0] >= N_SPLITS_CV and y_cv_post.nunique() == 2:\n",
        "                 try:\n",
        "                     all_splits_valid = all(len(np.unique(y_cv_post.iloc[test_idx])) >= 2 for _, test_idx in cv_splitter.split(X_cv_post, y_cv_post))\n",
        "                     if all_splits_valid: can_cv = True\n",
        "                 except Exception: can_cv = False\n",
        "            else: can_cv = False\n",
        "\n",
        "            if can_cv:\n",
        "                try:\n",
        "                    cv_scores = cross_val_score(cv_pipeline, X_cv_post, y_cv_post, cv=cv_splitter, scoring='roc_auc', n_jobs=-1)\n",
        "                    if np.all(np.isfinite(cv_scores)): mean_auc = cv_scores.mean(); std_auc = cv_scores.std()\n",
        "                    else: mean_auc = np.nan; std_auc = np.nan\n",
        "                except Exception as e: print(f\"Warning: CV failed for model index {index}: {e}\"); mean_auc=np.nan; std_auc=np.nan\n",
        "\n",
        "            cv_results_post.append({'index': index, 'num_vars': row['num_vars'], 'variables': row['variables'], 'training_auc': row['auc'], 'max_vif': row['max_vif'], 'mean_cv_auc': mean_auc, 'std_cv_auc': std_auc, 'cv_scores': cv_scores})\n",
        "        end_time_post_cv = time.time()\n",
        "        print(f\"Cross-validation for Top {len(top_n_post)} models took {end_time_post_cv - start_time_post_cv:.2f} seconds.\")\n",
        "\n",
        "        if cv_results_post:\n",
        "            cv_results_post_df = pd.DataFrame(cv_results_post).sort_values(by='mean_cv_auc', ascending=False, na_position='last')\n",
        "            print(f\"\\n--- Top {len(cv_results_post_df)} Models Ranked by Mean CV AUC (Post-operative, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "            display(cv_results_post_df[['num_vars', 'variables', 'training_auc', 'mean_cv_auc', 'std_cv_auc', 'max_vif']].round(4))\n",
        "\n",
        "            best_cv_post_row = cv_results_post_df[cv_results_post_df['mean_cv_auc'].notna()].iloc[0] if not cv_results_post_df[cv_results_post_df['mean_cv_auc'].notna()].empty else None\n",
        "\n",
        "            if best_cv_post_row is not None:\n",
        "                best_cv_model_post_info = best_cv_post_row.to_dict()\n",
        "                original_model_index = best_cv_model_post_info['index']\n",
        "                # Get model and vars from the original results df using the index\n",
        "                selected_post_model_fit = post_results_df.loc[original_model_index, 'model']\n",
        "                selected_post_vars = post_results_df.loc[original_model_index, 'variables'].split(', ') # Use original variables string\n",
        "\n",
        "                print(f\"\\n--- Best Post-operative Model (Selected by Highest Mean CV AUC, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "                print(f\"Variables ({best_cv_model_post_info['num_vars']}): {', '.join(selected_post_vars)}\")\n",
        "                print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {best_cv_model_post_info['mean_cv_auc']:.4f} (+/- {best_cv_model_post_info['std_cv_auc']:.4f})\")\n",
        "                print(f\"Original Training AUC: {best_cv_model_post_info['training_auc']:.4f}\")\n",
        "                print(f\"Max VIF (for info): {best_cv_model_post_info['max_vif']:.3f}\")\n",
        "\n",
        "                # 最終モデルの詳細評価\n",
        "                if selected_post_model_fit:\n",
        "                     print(\"\\n--- Detailed Collinearity Check (Final Selected Post-op Model) ---\")\n",
        "                     _ = check_collinearity(df_analysis, selected_post_vars, vif_threshold=VIF_THRESHOLD_FINAL_CHECK, output_dir=output_dir)\n",
        "                     print(\"\\n--- Summary (Final Selected Post-op Model, Refit on Full Data) ---\")\n",
        "                     try:\n",
        "                         # Use the stored model fit object for summary\n",
        "                         if selected_post_model_fit.mle_retvals['converged']:\n",
        "                             xnames_post_final = ['Intercept'] + selected_post_vars; print(selected_post_model_fit.summary(xname=xnames_post_final))\n",
        "                             print(\"\\nOdds Ratios:\")\n",
        "                             conf_final_post = selected_post_model_fit.conf_int(); conf_final_post['Odds Ratio'] = selected_post_model_fit.params; conf_final_post.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                             display(np.exp(conf_final_post).round(3))\n",
        "                         else:\n",
        "                              print(\"Stored model fit did not converge. Attempting to refit...\")\n",
        "                              raise ValueError(\"Stored model did not converge\") # Trigger refit below\n",
        "                     except Exception as e:\n",
        "                         print(f\"Could not display summary from stored model ({e}). Attempting refit...\")\n",
        "                         try:\n",
        "                             df_refit_post = df_analysis[[target_col] + selected_post_vars].dropna()\n",
        "                             y_refit_post = df_refit_post[target_col]; X_refit_post = sm.add_constant(df_refit_post[selected_post_vars])\n",
        "                             refit_post_model = sm.Logit(y_refit_post, X_refit_post).fit(disp=False)\n",
        "                             xnames_post_final = ['Intercept'] + selected_post_vars; print(refit_post_model.summary(xname=xnames_post_final))\n",
        "                             print(\"\\nOdds Ratios:\")\n",
        "                             conf_final_post = refit_post_model.conf_int(); conf_final_post['Odds Ratio'] = refit_post_model.params; conf_final_post.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                             display(np.exp(conf_final_post).round(3))\n",
        "                         except Exception as e_refit:\n",
        "                             print(f\"Refitting failed: {e_refit}\")\n",
        "                else: print(\"Original model object not found.\")\n",
        "            else: print(\"\\nCould not select a best model based on CV AUC (all failed or NaN).\")\n",
        "        else: print(\"\\nCross-validation could not be performed for any top model.\")\n",
        "    else: print(\"\\nWarning: No valid models found from the initial best subset search.\")\n",
        "\n",
        "\n",
        "# --- 5b. 術前モデル ---\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(\"=== Pre-operative Model: Required Vars + CV Selection ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "preop_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\",\n",
        "                        \"MRD-2 pre\", \"BUT pre\"]\n",
        "preop_candidate_cols_effective = list(set(preop_candidate_cols) | set(REQUIRED_VARIABLE))\n",
        "print(f\"Pre-operative candidate variables ({len(preop_candidate_cols_effective)}): {', '.join(sorted(preop_candidate_cols_effective))}\")\n",
        "\n",
        "\n",
        "cv_results_preop = []\n",
        "best_cv_model_preop_info = None\n",
        "preop_results_df = pd.DataFrame()\n",
        "\n",
        "if not all(req_var in df_analysis.columns for req_var in REQUIRED_VARIABLE):\n",
        "    print(f\"Error: Not all required variables {REQUIRED_VARIABLE} exist in the dataframe. Skipping pre-operative analysis.\")\n",
        "else:\n",
        "    start_time_preop_search = time.time()\n",
        "    # ★★★ 必須変数指定で関数呼び出し ★★★\n",
        "    preop_results_df = run_best_subset_logit_required(df_analysis, target_col, preop_candidate_cols_effective, required_cols=REQUIRED_VARIABLE)\n",
        "    end_time_preop_search = time.time()\n",
        "    print(f\"\\nBest subset search took {end_time_preop_search - start_time_preop_search:.2f} seconds.\")\n",
        "\n",
        "    valid_models_preop = preop_results_df[preop_results_df['error'].isna() & preop_results_df['auc'].notna()].copy()\n",
        "\n",
        "    if not valid_models_preop.empty:\n",
        "        top_n_preop = valid_models_preop.head(TOP_N_MODELS)\n",
        "        print(f\"\\n--- Performing {N_SPLITS_CV}-Fold CV on Top {len(top_n_preop)} Training AUC Models (Pre-operative, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "\n",
        "        start_time_preop_cv = time.time()\n",
        "        for index, row in tqdm(top_n_preop.iterrows(), total=len(top_n_preop), desc=\"CV on Top Models\"):\n",
        "            current_vars = row['variables'].split(', ')\n",
        "            df_cv_preop = df_analysis[[target_col] + current_vars].dropna()\n",
        "            y_cv_preop = df_cv_preop[target_col]\n",
        "            X_cv_preop = df_cv_preop[current_vars]\n",
        "            mean_auc = np.nan; std_auc = np.nan; cv_scores = []\n",
        "\n",
        "            can_cv = False\n",
        "            if X_cv_preop.shape[0] >= N_SPLITS_CV and y_cv_preop.nunique() == 2:\n",
        "                 try:\n",
        "                     all_splits_valid = all(len(np.unique(y_cv_preop.iloc[test_idx])) >= 2 for _, test_idx in cv_splitter.split(X_cv_preop, y_cv_preop))\n",
        "                     if all_splits_valid: can_cv = True\n",
        "                 except Exception: can_cv = False\n",
        "            else: can_cv = False\n",
        "\n",
        "            if can_cv:\n",
        "                try:\n",
        "                    cv_scores = cross_val_score(cv_pipeline, X_cv_preop, y_cv_preop, cv=cv_splitter, scoring='roc_auc', n_jobs=-1)\n",
        "                    if np.all(np.isfinite(cv_scores)): mean_auc = cv_scores.mean(); std_auc = cv_scores.std()\n",
        "                    else: mean_auc = np.nan; std_auc = np.nan\n",
        "                except Exception as e: print(f\"Warning: CV failed for model index {index}: {e}\"); mean_auc=np.nan; std_auc=np.nan\n",
        "\n",
        "            cv_results_preop.append({'index': index, 'num_vars': row['num_vars'], 'variables': row['variables'], 'training_auc': row['auc'], 'max_vif': row['max_vif'], 'mean_cv_auc': mean_auc, 'std_cv_auc': std_auc, 'cv_scores': cv_scores})\n",
        "        end_time_preop_cv = time.time()\n",
        "        print(f\"Cross-validation for Top {len(top_n_preop)} models took {end_time_preop_cv - start_time_preop_cv:.2f} seconds.\")\n",
        "\n",
        "        if cv_results_preop:\n",
        "            cv_results_preop_df = pd.DataFrame(cv_results_preop).sort_values(by='mean_cv_auc', ascending=False, na_position='last')\n",
        "            print(f\"\\n--- Top {len(cv_results_preop_df)} Models Ranked by Mean CV AUC (Pre-operative, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "            display(cv_results_preop_df[['num_vars', 'variables', 'training_auc', 'mean_cv_auc', 'std_cv_auc', 'max_vif']].round(4))\n",
        "\n",
        "            best_cv_preop_row = cv_results_preop_df[cv_results_preop_df['mean_cv_auc'].notna()].iloc[0] if not cv_results_preop_df[cv_results_preop_df['mean_cv_auc'].notna()].empty else None\n",
        "\n",
        "            if best_cv_preop_row is not None:\n",
        "                best_cv_model_preop_info = best_cv_preop_row.to_dict()\n",
        "                original_model_index_preop = best_cv_model_preop_info['index']\n",
        "                selected_preop_model_fit = preop_results_df.loc[original_model_index_preop, 'model']\n",
        "                selected_preop_vars = best_cv_model_preop_info['variables'].split(', ') # Use variables from CV results row\n",
        "\n",
        "                print(f\"\\n--- Best Pre-operative Model (Selected by Highest Mean CV AUC, requiring {REQUIRED_VARIABLE}) ---\")\n",
        "                print(f\"Variables ({best_cv_model_preop_info['num_vars']}): {', '.join(selected_preop_vars)}\")\n",
        "                print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {best_cv_model_preop_info['mean_cv_auc']:.4f} (+/- {best_cv_model_preop_info['std_cv_auc']:.4f})\")\n",
        "                print(f\"Original Training AUC: {best_cv_model_preop_info['training_auc']:.4f}\")\n",
        "                print(f\"Max VIF (for info): {best_cv_model_preop_info['max_vif']:.3f}\")\n",
        "\n",
        "                # 最終モデルの詳細評価\n",
        "                if selected_preop_model_fit:\n",
        "                     print(\"\\n--- Detailed Collinearity Check (Final Selected Pre-op Model) ---\")\n",
        "                     _ = check_collinearity(df_analysis, selected_preop_vars, vif_threshold=VIF_THRESHOLD_FINAL_CHECK, output_dir=output_dir)\n",
        "                     print(\"\\n--- Summary (Final Selected Pre-op Model, Refit on Full Data) ---\")\n",
        "                     try:\n",
        "                         # Use the stored model fit object for summary\n",
        "                         if selected_preop_model_fit.mle_retvals['converged']:\n",
        "                             xnames_preop_final = ['Intercept'] + selected_preop_vars; print(selected_preop_model_fit.summary(xname=xnames_preop_final))\n",
        "                             print(\"\\nOdds Ratios:\")\n",
        "                             conf_final_preop = selected_preop_model_fit.conf_int(); conf_final_preop['Odds Ratio'] = selected_preop_model_fit.params; conf_final_preop.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                             display(np.exp(conf_final_preop).round(3))\n",
        "                         else:\n",
        "                              print(\"Stored model fit did not converge. Attempting to refit...\")\n",
        "                              raise ValueError(\"Stored model did not converge\")\n",
        "                     except Exception as e:\n",
        "                         print(f\"Could not display summary from stored model ({e}). Attempting refit...\")\n",
        "                         try:\n",
        "                            df_refit_preop = df_analysis[[target_col] + selected_preop_vars].dropna()\n",
        "                            y_refit_preop = df_refit_preop[target_col]; X_refit_preop = sm.add_constant(df_refit_preop[selected_preop_vars])\n",
        "                            refit_preop_model = sm.Logit(y_refit_preop, X_refit_preop).fit(disp=False)\n",
        "                            xnames_preop_final = ['Intercept'] + selected_preop_vars; print(refit_preop_model.summary(xname=xnames_preop_final))\n",
        "                            print(\"\\nOdds Ratios:\")\n",
        "                            conf_final_preop = refit_preop_model.conf_int(); conf_final_preop['Odds Ratio'] = refit_preop_model.params; conf_final_preop.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "                            display(np.exp(conf_final_preop).round(3))\n",
        "                         except Exception as e_refit: print(f\"Refitting failed: {e_refit}\")\n",
        "                else: print(\"Original model object not found.\")\n",
        "            else: print(\"\\nCould not select a best model based on CV AUC (all failed or NaN).\")\n",
        "        else: print(\"\\nCross-validation could not be performed for any top model.\")\n",
        "    else: print(\"\\nWarning: No valid models found from the initial best subset search.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. 最終結果サマリー\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(f\"=== Final Model Summary (Selected by Best Mean {N_SPLITS_CV}-Fold CV AUC from Top {TOP_N_MODELS}, requiring {REQUIRED_VARIABLE}) ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "print(\"\\n--- Post-operative Model ---\")\n",
        "if best_cv_model_post_info:\n",
        "    final_post_vars = best_cv_model_post_info['variables'].split(', ') # Use info from CV results\n",
        "    print(f\"Selected Variables ({best_cv_model_post_info['num_vars']}): {', '.join(final_post_vars)}\")\n",
        "    print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {best_cv_model_post_info['mean_cv_auc']:.4f} (+/- {best_cv_model_post_info['std_cv_auc']:.4f})\")\n",
        "    print(f\"Original Training AUC: {best_cv_model_post_info['training_auc']:.4f}\")\n",
        "    print(f\"Max VIF (for info): {best_cv_model_post_info['max_vif']:.3f}\")\n",
        "else:\n",
        "    print(\"No suitable model selected.\")\n",
        "\n",
        "print(\"\\n--- Pre-operative Model ---\")\n",
        "if best_cv_model_preop_info:\n",
        "    final_preop_vars = best_cv_model_preop_info['variables'].split(', ') # Use info from CV results\n",
        "    print(f\"Selected Variables ({best_cv_model_preop_info['num_vars']}): {', '.join(final_preop_vars)}\")\n",
        "    print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {best_cv_model_preop_info['mean_cv_auc']:.4f} (+/- {best_cv_model_preop_info['std_cv_auc']:.4f})\")\n",
        "    print(f\"Original Training AUC: {best_cv_model_preop_info['training_auc']:.4f}\")\n",
        "    print(f\"Max VIF (for info): {best_cv_model_preop_info['max_vif']:.3f}\")\n",
        "else:\n",
        "    print(\"No suitable model selected.\")\n",
        "\n",
        "print(\"\\nReminder: Compare Training AUC and Mean CV AUC to assess potential overfitting.\")\n",
        "print(\"Review the final selected model's summary and collinearity check (especially VIF) for interpretation.\")\n",
        "print(f\"\\nAnalysis Complete. Check '{output_dir}' for correlation heatmaps.\")"
      ],
      "metadata": {
        "id": "hEE32BZIWc5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**スコアリングシステム構築**"
      ],
      "metadata": {
        "id": "SG2vc8uoWiUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：ForestPlotベースのスコアリングシステム構築と評価 Ver.4 (通し修正版)\n",
        "# =============================================================\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. ライブラリのインポートと基本設定\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "warnings.filterwarnings('ignore', message=\"Method 'first' requires numeric dtype to function correctly\") # 追加\n",
        "\n",
        "print(\"Libraries imported and warnings configured.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Google Drive マウント と ファイル/フォルダ設定\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    # ★★★ Google Drive内のExcelファイルのパスを指定してください ★★★\n",
        "    file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    # ★★★ 結果を出力するフォルダを指定してください ★★★\n",
        "    output_dir = \"/content/scoring_system_evaluation_results_v4\" # 新しいフォルダ名\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Running in a non-Colab environment.\")\n",
        "    # ★★★ ローカルのExcelファイルのパスを指定してください ★★★\n",
        "    file_path = \"眼瞼下垂★ドライアイ_kit.xlsx\" # 例\n",
        "    # ★★★ 結果を出力するフォルダを指定してください ★★★\n",
        "    output_dir = \"./scoring_system_evaluation_results_v4\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. データ読み込みと前処理\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 2. Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "    print(f\"Excel file loaded successfully. Original shape: {df_all.shape}\")\n",
        "except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); exit()\n",
        "except Exception as e: print(f\"Error loading Excel file: {e}\"); exit()\n",
        "\n",
        "cols_to_convert = [\"MRD-1 pre\", \"MRD-1 3M\", \"MRD-2 pre\", \"MRD-2 3M\", \"levator_function pre\", \"BUT pre\", \"BUT post\", \"SPK pre\", \"SPK post\"]\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns: df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "    else: print(f\"Warning: Column '{col}' not found.\")\n",
        "if \"MRD-1 3M\" in df_all.columns and \"MRD-1 pre\" in df_all.columns: df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "else: df_all[\"ΔMRD-1\"] = np.nan\n",
        "if \"MRD-2 3M\" in df_all.columns and \"MRD-2 pre\" in df_all.columns: df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "else: df_all[\"ΔMRD-2\"] = np.nan\n",
        "if \"SPK pre\" not in df_all.columns: print(\"Error: 'SPK pre' column not found.\"); exit()\n",
        "df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "if \"SPK post\" not in df.columns: print(\"Error: 'SPK post' column not found.\"); exit()\n",
        "df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "target_col = 'SPK post'\n",
        "print(f\"Analysis data shape (SPK pre=0, SPK post=0 or 1): {df_analysis.shape}\")\n",
        "if df_analysis.empty or df_analysis[target_col].nunique() < 2: print(\"Error: Insufficient data for analysis.\"); exit()\n",
        "print(f\"Target variable '{target_col}' distribution:\\n{df_analysis[target_col].value_counts()}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. スコアリングルールと候補変数の定義\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 3. Defining Scoring Rules and Candidate Factors ---\")\n",
        "scoring_rules = {\n",
        "    \"MRD-1 pre\":            {\"condition\": lambda x: pd.notna(x) and x <= 0.5, \"points\": 2, \"description\": \"MRD-1 pre <= 0.5 mm\"},\n",
        "    \"ΔMRD-1\":               {\"condition\": lambda x: pd.notna(x) and x >= 2.0, \"points\": 1, \"description\": \"ΔMRD-1 >= 2.0 mm\"},\n",
        "    \"levator_function pre\": {\"condition\": lambda x: pd.notna(x) and x < 8.0,  \"points\": 1, \"description\": \"Levator Function < 8.0 mm\"},\n",
        "    \"BUT pre\":              {\"condition\": lambda x: pd.notna(x) and x < 5.0,  \"points\": 1, \"description\": \"BUT pre < 5.0 sec\"},\n",
        "    \"BUT post\":             {\"condition\": lambda x: pd.notna(x) and x < 4.0,  \"points\": 1, \"description\": \"BUT post < 4.0 sec\"}\n",
        "}\n",
        "print(\"Scoring rules defined:\")\n",
        "for factor, rule in scoring_rules.items():\n",
        "    print(f\"  - {factor}: Add {rule['points']} point(s) if {rule['description']}\")\n",
        "\n",
        "preop_score_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT pre\"]\n",
        "preop_score_candidate_cols_valid = [col for col in preop_score_candidate_cols if col in df_analysis.columns and col in scoring_rules]\n",
        "print(f\"\\nValid Pre-operative score candidate factors: {preop_score_candidate_cols_valid}\")\n",
        "\n",
        "postop_score_candidate_cols = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT pre\", \"BUT post\"]\n",
        "postop_score_candidate_cols_valid = [col for col in postop_score_candidate_cols if col in df_analysis.columns and col in scoring_rules]\n",
        "print(f\"Valid Post-operative score candidate factors: {postop_score_candidate_cols_valid}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. スコアリングシステム評価関数\n",
        "# -------------------------------------------------------------\n",
        "def evaluate_scoring_systems(df_input, target_variable, candidate_factors_list, rules_dict, desc_str=\"\"):\n",
        "    score_system_results_list = []\n",
        "    print(f\"\\nEvaluating scoring systems for {desc_str} with candidates: {candidate_factors_list}\")\n",
        "\n",
        "    for k in tqdm(range(1, len(candidate_factors_list) + 1), desc=f\"Scoring Systems ({desc_str})\"):\n",
        "        for current_factor_subset_tuple in combinations(candidate_factors_list, k):\n",
        "            current_factors_in_subset = list(current_factor_subset_tuple)\n",
        "\n",
        "            def calculate_total_score_for_row(row_data, selected_vars, defined_rules):\n",
        "                total_score_val = 0\n",
        "                for var_name_in_subset in selected_vars:\n",
        "                    if var_name_in_subset in defined_rules and var_name_in_subset in row_data:\n",
        "                        if defined_rules[var_name_in_subset][\"condition\"](row_data[var_name_in_subset]):\n",
        "                            total_score_val += defined_rules[var_name_in_subset][\"points\"]\n",
        "                return total_score_val\n",
        "\n",
        "            temp_df_with_score = df_input.copy()\n",
        "            score_col_name = f\"temp_total_score\" # ループ内で一時的に使う列名\n",
        "            temp_df_with_score[score_col_name] = temp_df_with_score.apply(\n",
        "                lambda r: calculate_total_score_for_row(r, current_factors_in_subset, defined_rules=rules_dict), axis=1\n",
        "            )\n",
        "            df_model_eval_current = temp_df_with_score[[target_variable, score_col_name]].dropna()\n",
        "\n",
        "            if df_model_eval_current.shape[0] < 10 or df_model_eval_current[target_variable].nunique() < 2 or df_model_eval_current[score_col_name].nunique() < 2:\n",
        "                score_system_results_list.append({'score_factors': ', '.join(current_factors_in_subset), 'num_score_factors': len(current_factors_in_subset), 'training_auc_of_score_model': np.nan, 'model_object':None, 'error': 'Insufficient data'})\n",
        "                continue\n",
        "\n",
        "            y_eval_current = df_model_eval_current[target_variable]\n",
        "            X_eval_score_current = sm.add_constant(df_model_eval_current[[score_col_name]])\n",
        "            auc_val_current = np.nan; error_msg_current = None; model_fit_current = None\n",
        "            try:\n",
        "                score_logit_model = sm.Logit(y_eval_current, X_eval_score_current)\n",
        "                model_fit_current = score_logit_model.fit(disp=False, maxiter=100, warn_convergence=False) # warn_convergence追加\n",
        "                if model_fit_current.mle_retvals['converged']:\n",
        "                    y_pred_prob_score = model_fit_current.predict(X_eval_score_current)\n",
        "                    auc_val_current = roc_auc_score(y_eval_current, y_pred_prob_score)\n",
        "                else: error_msg_current = \"Convergence Failed\"\n",
        "            except Exception as e_current: error_msg_current = str(e_current)\n",
        "            score_system_results_list.append({'score_factors': ', '.join(current_factors_in_subset), 'num_score_factors': len(current_factors_in_subset), 'training_auc_of_score_model': auc_val_current, 'model_object': model_fit_current, 'error': error_msg_current})\n",
        "\n",
        "    results_df_current = pd.DataFrame(score_system_results_list).sort_values(by='training_auc_of_score_model', ascending=False, na_position='last').reset_index(drop=True)\n",
        "    return results_df_current\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. メイン処理: スコアリングシステムの構築とCV評価\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 5. Main Processing: Scoring System Construction and CV Evaluation ---\")\n",
        "N_SPLITS_CV = 5\n",
        "TOP_N_SCORING_SYSTEMS = 10\n",
        "cv_pipeline_score = Pipeline([('scaler', StandardScaler()), ('logistic', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))])\n",
        "cv_splitter_score = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=42)\n",
        "\n",
        "# --- 5a. 術前スコアリングシステム ---\n",
        "print(\"\\n======================================================\")\n",
        "print(\"=== Pre-operative Scoring System Evaluation ===\")\n",
        "print(\"======================================================\")\n",
        "best_preop_scoring_system_info = None\n",
        "preop_scoring_results_df = pd.DataFrame()\n",
        "if preop_score_candidate_cols_valid:\n",
        "    preop_scoring_results_df = evaluate_scoring_systems(df_analysis, target_col, preop_score_candidate_cols_valid, scoring_rules, desc_str=\"PreOp\")\n",
        "    print(f\"\\n--- Top Scoring Systems by Training AUC (Pre-operative) ---\")\n",
        "    display(preop_scoring_results_df[['score_factors', 'num_score_factors', 'training_auc_of_score_model', 'error']].head(TOP_N_SCORING_SYSTEMS).round(4))\n",
        "    top_n_preop_scores = preop_scoring_results_df[preop_scoring_results_df['error'].isna() & preop_scoring_results_df['training_auc_of_score_model'].notna()].head(TOP_N_SCORING_SYSTEMS)\n",
        "    cv_eval_results_preop = []\n",
        "    if not top_n_preop_scores.empty:\n",
        "        print(f\"\\n--- Performing {N_SPLITS_CV}-Fold CV on Top {len(top_n_preop_scores)} Pre-operative Scoring Systems ---\")\n",
        "        for index, row_data in tqdm(top_n_preop_scores.iterrows(), total=len(top_n_preop_scores), desc=\"CV on PreOp Scores\"):\n",
        "            current_factors_list = row_data['score_factors'].split(', ')\n",
        "            temp_df_cv = df_analysis.copy()\n",
        "            temp_df_cv['current_total_score'] = temp_df_cv.apply(lambda r_cv: sum(scoring_rules[f_cv]['points'] for f_cv in current_factors_list if f_cv in scoring_rules and f_cv in r_cv and scoring_rules[f_cv]['condition'](r_cv[f_cv])), axis=1)\n",
        "            df_cv_eval_current = temp_df_cv[[target_col, 'current_total_score']].dropna()\n",
        "            y_cv_current = df_cv_eval_current[target_col]; X_cv_score_current = df_cv_eval_current[['current_total_score']]\n",
        "            mean_auc_current = np.nan; std_auc_current = np.nan\n",
        "            if X_cv_score_current.shape[0] >= N_SPLITS_CV and y_cv_current.nunique() == 2 and X_cv_score_current['current_total_score'].nunique() >=2 :\n",
        "                try:\n",
        "                    cv_scores_current = cross_val_score(cv_pipeline_score, X_cv_score_current, y_cv_current, cv=cv_splitter_score, scoring='roc_auc', n_jobs=-1)\n",
        "                    if np.all(np.isfinite(cv_scores_current)): mean_auc_current = cv_scores_current.mean(); std_auc_current = cv_scores_current.std()\n",
        "                except Exception as e_cv: print(f\"CV Error for pre-op score ({row_data['score_factors']}): {e_cv}\")\n",
        "            cv_eval_results_preop.append({'original_index': index, 'score_factors': row_data['score_factors'], 'num_score_factors': row_data['num_score_factors'], 'training_auc': row_data['training_auc_of_score_model'], 'mean_cv_auc': mean_auc_current, 'std_cv_auc': std_auc_current})\n",
        "        if cv_eval_results_preop:\n",
        "            cv_eval_results_preop_df = pd.DataFrame(cv_eval_results_preop).sort_values(by='mean_cv_auc', ascending=False, na_position='last')\n",
        "            print(f\"\\n--- Top Pre-operative Scoring Systems Ranked by Mean CV AUC ---\")\n",
        "            display(cv_eval_results_preop_df.head(10).round(4))\n",
        "            if not cv_eval_results_preop_df.empty and cv_eval_results_preop_df['mean_cv_auc'].notna().any():\n",
        "                 best_preop_scoring_system_info = cv_eval_results_preop_df.iloc[0].to_dict()\n",
        "    else: print(\"No valid pre-operative scoring systems found for CV.\")\n",
        "else: print(\"No valid candidate factors for pre-operative scoring system.\")\n",
        "\n",
        "# --- 5b. 術後スコアリングシステム ---\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(\"=== Post-operative Scoring System Evaluation ===\")\n",
        "print(\"======================================================\")\n",
        "best_postop_scoring_system_info = None\n",
        "postop_scoring_results_df = pd.DataFrame()\n",
        "if postop_score_candidate_cols_valid:\n",
        "    postop_scoring_results_df = evaluate_scoring_systems(df_analysis, target_col, postop_score_candidate_cols_valid, scoring_rules, desc_str=\"PostOp\")\n",
        "    print(f\"\\n--- Top Scoring Systems by Training AUC (Post-operative) ---\")\n",
        "    display(postop_scoring_results_df[['score_factors', 'num_score_factors', 'training_auc_of_score_model', 'error']].head(TOP_N_SCORING_SYSTEMS).round(4))\n",
        "    top_n_postop_scores = postop_scoring_results_df[postop_scoring_results_df['error'].isna() & postop_scoring_results_df['training_auc_of_score_model'].notna()].head(TOP_N_SCORING_SYSTEMS)\n",
        "    cv_eval_results_postop = []\n",
        "    if not top_n_postop_scores.empty:\n",
        "        print(f\"\\n--- Performing {N_SPLITS_CV}-Fold CV on Top {len(top_n_postop_scores)} Post-operative Scoring Systems ---\")\n",
        "        for index, row_data in tqdm(top_n_postop_scores.iterrows(), total=len(top_n_postop_scores), desc=\"CV on PostOp Scores\"):\n",
        "            current_factors_list = row_data['score_factors'].split(', ')\n",
        "            temp_df_cv = df_analysis.copy()\n",
        "            temp_df_cv['current_total_score'] = temp_df_cv.apply(lambda r_cv: sum(scoring_rules[f_cv]['points'] for f_cv in current_factors_list if f_cv in scoring_rules and f_cv in r_cv and scoring_rules[f_cv]['condition'](r_cv[f_cv])), axis=1)\n",
        "            df_cv_eval_current = temp_df_cv[[target_col, 'current_total_score']].dropna()\n",
        "            y_cv_current = df_cv_eval_current[target_col]; X_cv_score_current = df_cv_eval_current[['current_total_score']]\n",
        "            mean_auc_current = np.nan; std_auc_current = np.nan\n",
        "            if X_cv_score_current.shape[0] >= N_SPLITS_CV and y_cv_current.nunique() == 2 and X_cv_score_current['current_total_score'].nunique() >=2 :\n",
        "                try:\n",
        "                    cv_scores_current = cross_val_score(cv_pipeline_score, X_cv_score_current, y_cv_current, cv=cv_splitter_score, scoring='roc_auc', n_jobs=-1)\n",
        "                    if np.all(np.isfinite(cv_scores_current)): mean_auc_current = cv_scores_current.mean(); std_auc_current = cv_scores_current.std()\n",
        "                except Exception as e_cv: print(f\"CV Error for post-op score ({row_data['score_factors']}): {e_cv}\")\n",
        "            cv_eval_results_postop.append({'original_index': index, 'score_factors': row_data['score_factors'], 'num_score_factors': row_data['num_score_factors'], 'training_auc': row_data['training_auc_of_score_model'], 'mean_cv_auc': mean_auc_current, 'std_cv_auc': std_auc_current})\n",
        "        if cv_eval_results_postop:\n",
        "            cv_eval_results_postop_df = pd.DataFrame(cv_eval_results_postop).sort_values(by='mean_cv_auc', ascending=False, na_position='last')\n",
        "            print(f\"\\n--- Top Post-operative Scoring Systems Ranked by Mean CV AUC ---\")\n",
        "            display(cv_eval_results_postop_df.head(10).round(4))\n",
        "            if not cv_eval_results_postop_df.empty and cv_eval_results_postop_df['mean_cv_auc'].notna().any():\n",
        "                best_postop_scoring_system_info = cv_eval_results_postop_df.iloc[0].to_dict()\n",
        "    else: print(\"No valid post-operative scoring systems found for CV.\")\n",
        "else: print(\"No valid candidate factors for post-operative scoring system.\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. 最終結果サマリーと詳細評価\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(f\"=== Final Scoring System Summary (Selected by Best Mean {N_SPLITS_CV}-Fold CV AUC) ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "def display_final_scoring_system_details(system_info, original_results_df_for_model_obj, df_analysis_data, target_col_name, rules_dict_local, desc_str=\"\"):\n",
        "    if system_info is None:\n",
        "        print(f\"\\nNo best {desc_str} scoring system selected.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n--- Best {desc_str} Scoring System ---\")\n",
        "    selected_score_factors_list = system_info['score_factors'].split(', ')\n",
        "    print(f\"Selected Score Factors ({system_info['num_score_factors']}): {', '.join(selected_score_factors_list)}\")\n",
        "    print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {system_info['mean_cv_auc']:.4f} (+/- {system_info['std_cv_auc']:.4f})\")\n",
        "    print(f\"Original Training AUC of Score Model: {system_info['training_auc']:.4f}\")\n",
        "\n",
        "    # Retrieve the original fitted model object if needed for summary, or refit\n",
        "    final_score_model_for_summary = None\n",
        "    if 'original_index' in system_info and original_results_df_for_model_obj is not None and not original_results_df_for_model_obj.empty:\n",
        "        try:\n",
        "            original_model_idx = system_info['original_index']\n",
        "            final_score_model_for_summary = original_results_df_for_model_obj.loc[original_model_idx, 'model_object']\n",
        "        except KeyError:\n",
        "            print(\"Warning: Could not retrieve original model object from results.\")\n",
        "\n",
        "\n",
        "    final_df_display = df_analysis_data.copy()\n",
        "    final_df_display['final_score'] = final_df_display.apply(\n",
        "        lambda r_disp: sum(rules_dict_local[f_disp]['points'] for f_disp in selected_score_factors_list if f_disp in rules_dict_local and f_disp in r_disp and rules_dict_local[f_disp]['condition'](r_disp[f_disp])), axis=1\n",
        "    )\n",
        "    df_final_model_eval_disp = final_df_display[[target_col_name, 'final_score']].dropna()\n",
        "\n",
        "    if not df_final_model_eval_disp.empty and df_final_model_eval_disp['final_score'].nunique() > 0:\n",
        "        print(f\"\\n--- Score Distribution and SPK Incidence for Best {desc_str} Scoring System ---\")\n",
        "        score_summary_disp = df_final_model_eval_disp.groupby('final_score')[target_col_name].agg(\n",
        "            Total_Cases='count', SPK_Positive_Cases='sum', SPK_Rate='mean'\n",
        "        ).reset_index()\n",
        "        score_summary_disp['SPK_Rate_Percent'] = (score_summary_disp['SPK_Rate'] * 100).round(1)\n",
        "        display(score_summary_disp)\n",
        "\n",
        "        plt.figure(figsize=(max(6, score_summary_disp['final_score'].nunique() * 0.8), 4))\n",
        "        sns.countplot(x='final_score', data=df_final_model_eval_disp, palette=\"viridis\", order = sorted(df_final_model_eval_disp['final_score'].unique()))\n",
        "        plt.title(f\"Case Count per Score Point ({desc_str} System)\"); plt.xlabel(\"Score Point\"); plt.ylabel(\"Number of Cases\"); plt.tight_layout()\n",
        "        count_fig_path_disp = os.path.join(output_dir, f\"Countplot_Final_{desc_str.replace(' ','_')}_Score.png\")\n",
        "        plt.savefig(count_fig_path_disp, dpi=300); print(f\"Score count plot saved to {count_fig_path_disp}\"); plt.show()\n",
        "\n",
        "        if not score_summary_disp.empty:\n",
        "            plt.figure(figsize=(max(6, score_summary_disp['final_score'].nunique() * 0.8), 4))\n",
        "            barplot = sns.barplot(x='final_score', y='SPK_Rate', data=score_summary_disp, palette=\"magma\", order = sorted(score_summary_disp['final_score'].unique()))\n",
        "            plt.title(f\"SPK Incidence Rate per Score Point ({desc_str} System)\"); plt.xlabel(\"Score Point\"); plt.ylabel(\"SPK Incidence Rate\")\n",
        "            plt.ylim(0, max(1.05, score_summary_disp['SPK_Rate'].max() * 1.1 if not score_summary_disp['SPK_Rate'].empty else 1.05))\n",
        "            for i_disp, patch_disp in enumerate(barplot.patches):\n",
        "                 try:\n",
        "                     current_score_val_disp = sorted(score_summary_disp['final_score'].unique())[i_disp]\n",
        "                     rate_percent_disp = score_summary_disp[score_summary_disp['final_score'] == current_score_val_disp]['SPK_Rate_Percent'].iloc[0]\n",
        "                     bar_x_pos_disp = patch_disp.get_x() + patch_disp.get_width() / 2\n",
        "                     barplot.text(bar_x_pos_disp, patch_disp.get_height() + 0.02, f\"{rate_percent_disp:.1f}%\", color='black', ha=\"center\", va=\"bottom\")\n",
        "                 except (IndexError, KeyError): print(f\"Warning: Could not annotate bar for score (index {i_disp}).\")\n",
        "            plt.tight_layout(); rate_fig_path_disp = os.path.join(output_dir, f\"Rateplot_Final_{desc_str.replace(' ','_')}_Score.png\")\n",
        "            plt.savefig(rate_fig_path_disp, dpi=300); print(f\"SPK rate plot saved to {rate_fig_path_disp}\"); plt.show()\n",
        "        else: print(\"Could not generate SPK rate summary for the plot.\")\n",
        "    else: print(f\"Not enough data or score variation to display score distribution for {desc_str} system.\")\n",
        "\n",
        "    if df_final_model_eval_disp.shape[0] > 10 and df_final_model_eval_disp[target_col_name].nunique() == 2 and df_final_model_eval_disp['final_score'].nunique() >=2 :\n",
        "        y_final_disp = df_final_model_eval_disp[target_col_name]; X_final_score_disp = sm.add_constant(df_final_model_eval_disp[['final_score']])\n",
        "        try:\n",
        "            if final_score_model_for_summary and final_score_model_for_summary.mle_retvals['converged']:\n",
        "                print(f\"\\n--- Summary (Logistic Regression on Final {desc_str} Score - Using Stored Model) ---\")\n",
        "                final_model_to_show = final_score_model_for_summary\n",
        "            else: # Refit if stored model is not good or not available\n",
        "                print(f\"\\n--- Summary (Logistic Regression on Final {desc_str} Score - Refitting Model) ---\")\n",
        "                final_model_to_show = sm.Logit(y_final_disp, X_final_score_disp).fit(disp=False)\n",
        "\n",
        "            print(final_model_to_show.summary(xname=['Intercept', 'Final Score']))\n",
        "            print(\"\\nOdds Ratios (per 1 point increase in score):\")\n",
        "            conf_final_disp = final_model_to_show.conf_int(); conf_final_disp['Odds Ratio'] = final_model_to_show.params\n",
        "            conf_final_disp.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']; display(np.exp(conf_final_disp).round(3))\n",
        "            y_prob_final_disp = final_model_to_show.predict(X_final_score_disp); fpr_disp, tpr_disp, _ = roc_curve(y_final_disp, y_prob_final_disp)\n",
        "            auc_val_disp = roc_auc_score(y_final_disp, y_prob_final_disp); plt.figure(figsize=(5,5))\n",
        "            plt.plot(fpr_disp, tpr_disp, label=f\"Final {desc_str} Score Model\\nAUC = {auc_val_disp:.3f}\"); plt.plot([0,1],[0,1],'k--')\n",
        "            plt.xlabel(\"1 - Specificity\"); plt.ylabel(\"Sensitivity\"); plt.title(f\"ROC for Final {desc_str} Scoring System\")\n",
        "            plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "            roc_fig_path_disp = os.path.join(output_dir, f\"ROC_Final_{desc_str.replace(' ','_')}_Score.png\")\n",
        "            plt.savefig(roc_fig_path_disp, dpi=300); print(f\"ROC curve saved to {roc_fig_path_disp}\"); plt.show()\n",
        "        except Exception as e_final_disp: print(f\"Error displaying final {desc_str} score model summary/ROC: {e_final_disp}\")\n",
        "    else: print(f\"Not enough data to display final {desc_str} score model summary/ROC.\")\n",
        "\n",
        "    print(f\"\\nRules for the Best {desc_str} Scoring System:\")\n",
        "    for factor_name_disp in selected_score_factors_list:\n",
        "        if factor_name_disp in rules_dict_local and 'description' in rules_dict_local[factor_name_disp]:\n",
        "            print(f\"  - {factor_name_disp}: Add {rules_dict_local[factor_name_disp]['points']} point(s) if {rules_dict_local[factor_name_disp]['description']}.\")\n",
        "        elif factor_name_disp in rules_dict_local: print(f\"  - {factor_name_disp}: Add {rules_dict_local[factor_name_disp]['points']} point(s) if condition met.\")\n",
        "\n",
        "display_final_scoring_system_details(best_preop_scoring_system_info, preop_scoring_results_df, df_analysis, target_col, scoring_rules, desc_str=\"Pre-operative\")\n",
        "display_final_scoring_system_details(best_postop_scoring_system_info, postop_scoring_results_df, df_analysis, target_col, scoring_rules, desc_str=\"Post-operative\")\n",
        "\n",
        "print(\"\\nReminder: The selected scoring system's performance is based on the predefined rules.\")\n",
        "print(\"Further refinement of cutoffs or points might yield different results.\")\n",
        "print(f\"\\nAnalysis Complete. Check '{output_dir}' for ROC curves and score plots.\")"
      ],
      "metadata": {
        "id": "ny1-0nYrWkPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：固定因子セット評価版\n",
        "# =============================================================\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. ライブラリのインポートと基本設定\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# from itertools import combinations # combinations は不要に\n",
        "from tqdm.auto import tqdm # tqdmも不要になる可能性あり、状況に応じて残す\n",
        "# import joblib # joblibも使用箇所がなければ不要\n",
        "\n",
        "import statsmodels.api as sm\n",
        "# from statsmodels.stats.outliers_influence import variance_inflation_factor # VIFも不要に\n",
        "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve # make_scorerは不要に\n",
        "# from sklearn.model_selection import StratifiedKFold, cross_val_score # CVも今回省略\n",
        "from sklearn.linear_model import LogisticRegression # CV省略ならこれも不要かも\n",
        "from sklearn.preprocessing import StandardScaler # CV省略ならこれも不要かも\n",
        "from sklearn.pipeline import Pipeline # CV省略ならこれも不要かも\n",
        "\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "# import time # timeも使用箇所がなければ不要\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "warnings.filterwarnings('ignore', message=\"Method 'first' requires numeric dtype to function correctly\")\n",
        "\n",
        "print(\"Libraries imported and warnings configured.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Google Drive マウント と ファイル/フォルダ設定\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\" # ご自身のパスに修正\n",
        "    output_dir = \"/content/fixed_system_evaluation_results\"\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Running in a non-Colab environment.\")\n",
        "    file_path = \"眼瞼下垂★ドライアイ_kit.xlsx\" # ローカルパスの例\n",
        "    output_dir = \"./fixed_system_evaluation_results\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. データ読み込みと前処理\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 2. Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "    print(f\"Excel file loaded successfully. Original shape: {df_all.shape}\")\n",
        "except FileNotFoundError: print(f\"Error: File not found at {file_path}\"); exit()\n",
        "except Exception as e: print(f\"Error loading Excel file: {e}\"); exit()\n",
        "\n",
        "cols_to_convert = [\"MRD-1 pre\", \"MRD-1 3M\", \"MRD-2 pre\", \"MRD-2 3M\", \"levator_function pre\", \"BUT pre\", \"BUT post\", \"SPK pre\", \"SPK post\"]\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns: df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "    else: print(f\"Warning: Column '{col}' not found.\")\n",
        "\n",
        "if \"MRD-1 3M\" in df_all.columns and \"MRD-1 pre\" in df_all.columns: df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "else: df_all[\"ΔMRD-1\"] = np.nan\n",
        "if \"MRD-2 3M\" in df_all.columns and \"MRD-2 pre\" in df_all.columns: df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "else: df_all[\"ΔMRD-2\"] = np.nan # この変数は今回の評価では使われない可能性\n",
        "\n",
        "if \"SPK pre\" not in df_all.columns: print(\"Error: 'SPK pre' column not found.\"); exit()\n",
        "df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "if \"SPK post\" not in df.columns: print(\"Error: 'SPK post' column not found.\"); exit()\n",
        "df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "target_col = 'SPK post'\n",
        "print(f\"Analysis data shape (SPK pre=0, SPK post=0 or 1): {df_analysis.shape}\")\n",
        "if df_analysis.empty or df_analysis[target_col].nunique() < 2: print(\"Error: Insufficient data for analysis.\"); exit()\n",
        "print(f\"Target variable '{target_col}' distribution:\\n{df_analysis[target_col].value_counts()}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. スコアリングルールと評価対象因子の定義\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 3. Defining Scoring Rules and Evaluation Factors ---\")\n",
        "\n",
        "# ★★★★★ ここで評価したいスコアリングルールを定義・変更してください ★★★★★\n",
        "scoring_rules = {\n",
        "    #\"MRD-1 pre\":          {\"condition\": lambda x: pd.notna(x) and x <= 0.5, \"points\": 2, \"description\": \"MRD-1 pre <= 0.5 mm\"},\n",
        "    \"ΔMRD-1\":             {\"condition\": lambda x: pd.notna(x) and x >= 2.5, \"points\": 1, \"description\": \"ΔMRD-1 >= 2.5 mm\"},\n",
        "    #\"ΔMRD-1\":             {\"condition\": lambda x: pd.notna(x) and x >= 100.0, \"points\": 1, \"description\": \"ΔMRD-1 >= 100.0 mm\"}, # テスト用\n",
        "    \"levator_function pre\": {\"condition\": lambda x: pd.notna(x) and x < 8.0,  \"points\": 1, \"description\": \"Levator Function < 8.0 mm\"},\n",
        "    \"BUT pre\":            {\"condition\": lambda x: pd.notna(x) and x < 5.0,  \"points\": 1, \"description\": \"BUT pre < 5.0 sec\"},\n",
        "    \"BUT post\":           {\"condition\": lambda x: pd.notna(x) and x < 4.0,  \"points\": 1, \"description\": \"BUT post < 4.0 sec\"}\n",
        "}\n",
        "print(\"Scoring rules to be used:\")\n",
        "for factor, rule in scoring_rules.items():\n",
        "    # ルールで定義されている全ての因子がデータに存在するか確認\n",
        "    if factor not in df_analysis.columns:\n",
        "        print(f\"Warning: Factor '{factor}' defined in scoring_rules is NOT present in the analysis data. It will be ignored if listed in evaluation factors.\")\n",
        "    print(f\"  - {factor}: Add {rule['points']} point(s) if {rule['description']}\")\n",
        "\n",
        "# ★★★★★ ここで評価したい固定の因子セットを定義してください ★★★★★\n",
        "# 例: 元のコードのベストだった術前・術後因子セットなど\n",
        "# 使用する因子名は scoring_rules のキー及び df_analysis の列名と一致させてください\n",
        "pre_operative_factors_to_evaluate = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT pre\"]\n",
        "post_operative_factors_to_evaluate = [\"ΔMRD-1\", \"levator_function pre\", \"BUT pre\", \"BUT post\"]\n",
        "\n",
        "# 実際にデータに存在する有効な因子のみにフィルタリング\n",
        "pre_operative_factors_valid = [f for f in pre_operative_factors_to_evaluate if f in df_analysis.columns and f in scoring_rules]\n",
        "post_operative_factors_valid = [f for f in post_operative_factors_to_evaluate if f in df_analysis.columns and f in scoring_rules]\n",
        "\n",
        "print(f\"\\nPre-operative factors for evaluation: {pre_operative_factors_valid}\")\n",
        "print(f\"Post-operative factors for evaluation: {post_operative_factors_valid}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. 固定因子セット評価関数\n",
        "# -------------------------------------------------------------\n",
        "def evaluate_fixed_factor_system(df_data, target_variable, fixed_factors_list, rules_dict, desc_str=\"\"):\n",
        "    \"\"\"\n",
        "    指定された固定因子セットとルールに基づいてスコアを計算し、\n",
        "    ロジスティック回帰モデルでAUCとスコアごとの確率を評価する。\n",
        "    \"\"\"\n",
        "    print(f\"\\n\\n======================================================\")\n",
        "    print(f\"=== Evaluating Fixed System: {desc_str} ===\")\n",
        "    print(f\"======================================================\")\n",
        "    print(f\"Using factors: {fixed_factors_list}\")\n",
        "    print(\"Applied rules:\")\n",
        "    for factor_name in fixed_factors_list:\n",
        "        if factor_name in rules_dict:\n",
        "            print(f\"  - {factor_name}: Add {rules_dict[factor_name]['points']} point(s) if {rules_dict[factor_name]['description']}\")\n",
        "        else:\n",
        "            print(f\"  - Warning: Factor '{factor_name}' is in fixed_factors_list but not in rules_dict.\")\n",
        "\n",
        "    if not fixed_factors_list:\n",
        "        print(\"Error: No valid factors provided for evaluation.\")\n",
        "        return\n",
        "\n",
        "    # --- スコア計算 ---\n",
        "    current_score_col = f'{desc_str.lower().replace(\" \", \"_\")}_score'\n",
        "    df_eval = df_data.copy()\n",
        "\n",
        "    def calculate_row_score(row, factors, rules):\n",
        "        score = 0\n",
        "        for factor in factors:\n",
        "            if factor in rules and factor in row and pd.notna(row[factor]): # 欠損値でないことも確認\n",
        "                try:\n",
        "                    if rules[factor]['condition'](row[factor]):\n",
        "                        score += rules[factor]['points']\n",
        "                except Exception as e:\n",
        "                    print(f\"Warning: Error applying condition for factor '{factor}' on row (index {row.name if hasattr(row, 'name') else 'N/A'}): {e}\")\n",
        "        return score\n",
        "\n",
        "    df_eval[current_score_col] = df_eval.apply(\n",
        "        lambda row: calculate_row_score(row, fixed_factors_list, rules_dict),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    df_model_input = df_eval[[target_variable, current_score_col]].dropna()\n",
        "\n",
        "    if df_model_input.shape[0] < 10 or df_model_input[target_variable].nunique() < 2 :\n",
        "        print(f\"Error: Insufficient data for model evaluation for {desc_str} (N={df_model_input.shape[0]}).\")\n",
        "        if df_model_input[target_variable].nunique() < 2:\n",
        "             print(f\"Target variable has less than 2 unique values: {df_model_input[target_variable].unique()}\")\n",
        "        return\n",
        "    if df_model_input[current_score_col].nunique() < 2 and df_model_input.shape[0] >=10 : # スコアが1種類しかない場合も注意\n",
        "        print(f\"Warning: Only one unique score value ({df_model_input[current_score_col].unique()}) calculated for {desc_str}. AUC might be meaningless or error.\")\n",
        "\n",
        "\n",
        "    # --- ロジスティック回帰モデルとAUC ---\n",
        "    y_true = df_model_input[target_variable]\n",
        "    X_score = sm.add_constant(df_model_input[[current_score_col]])\n",
        "    auc_value = np.nan\n",
        "    model_fit = None\n",
        "\n",
        "    print(f\"\\n--- Model Training and AUC for {desc_str} ---\")\n",
        "    try:\n",
        "        logit_model = sm.Logit(y_true, X_score)\n",
        "        model_fit = logit_model.fit(disp=False, maxiter=200) # maxiterを少し増やす\n",
        "        if model_fit.mle_retvals['converged']:\n",
        "            y_pred_prob = model_fit.predict(X_score)\n",
        "            if len(np.unique(y_true)) > 1: # AUC計算はターゲットが2クラス以上の場合のみ\n",
        "                 auc_value = roc_auc_score(y_true, y_pred_prob)\n",
        "                 print(f\"Model converged. Training AUC: {auc_value:.4f}\")\n",
        "            else:\n",
        "                 print(\"Model converged, but AUC cannot be calculated (target has only 1 class).\")\n",
        "\n",
        "            print(model_fit.summary(xname=['Intercept', 'Score']))\n",
        "\n",
        "            print(\"\\nOdds Ratios (per 1 point increase in score):\")\n",
        "            conf_int_df = model_fit.conf_int()\n",
        "            conf_int_df['Odds Ratio'] = model_fit.params\n",
        "            conf_int_df.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']\n",
        "            display(np.exp(conf_int_df).round(3))\n",
        "\n",
        "            if len(np.unique(y_true)) > 1:\n",
        "                fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
        "                plt.figure(figsize=(5,5))\n",
        "                plt.plot(fpr, tpr, label=f\"{desc_str} Score Model\\nAUC = {auc_value:.3f}\")\n",
        "                plt.plot([0,1],[0,1],'k--')\n",
        "                plt.xlabel(\"1 - Specificity\"); plt.ylabel(\"Sensitivity\")\n",
        "                plt.title(f\"ROC for {desc_str} Scoring System\")\n",
        "                plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "                roc_fig_path = os.path.join(output_dir, f\"ROC_Fixed_{desc_str.replace(' ','_')}_Score.png\")\n",
        "                plt.savefig(roc_fig_path, dpi=300); print(f\"ROC curve saved to {roc_fig_path}\"); plt.show()\n",
        "        else:\n",
        "            print(\"Error: Logistic regression model did not converge.\")\n",
        "            if model_fit: print(model_fit.mle_retvals)\n",
        "\n",
        "    except PerfectSeparationError:\n",
        "        print(\"Error: Perfect separation detected. Cannot fit logistic regression model.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during model fitting or AUC calculation: {e}\")\n",
        "\n",
        "    # --- スコアごとの確率（実績値）とグラフ ---\n",
        "    print(f\"\\n--- Score Distribution and SPK Incidence for {desc_str} ---\")\n",
        "    if df_model_input[current_score_col].nunique() > 0:\n",
        "        score_summary = df_model_input.groupby(current_score_col)[target_variable].agg(\n",
        "            Total_Cases='count', SPK_Positive_Cases='sum', SPK_Rate='mean'\n",
        "        ).reset_index()\n",
        "        score_summary['SPK_Rate_Percent'] = (score_summary['SPK_Rate'] * 100).round(1)\n",
        "\n",
        "        if score_summary.empty:\n",
        "            print(\"No data to display for score summary.\")\n",
        "        else:\n",
        "            display(score_summary)\n",
        "\n",
        "            # 症例数グラフ\n",
        "            plt.figure(figsize=(max(6, score_summary[current_score_col].nunique() * 0.8), 4))\n",
        "            sns.countplot(x=current_score_col, data=df_model_input, palette=\"viridis\", order = sorted(df_model_input[current_score_col].unique()))\n",
        "            plt.title(f\"Case Count per Score Point ({desc_str})\")\n",
        "            plt.xlabel(\"Score Point\"); plt.ylabel(\"Number of Cases\"); plt.tight_layout()\n",
        "            count_fig_path = os.path.join(output_dir, f\"Countplot_Fixed_{desc_str.replace(' ','_')}_Score.png\")\n",
        "            plt.savefig(count_fig_path, dpi=300); print(f\"Score count plot saved to {count_fig_path}\"); plt.show()\n",
        "\n",
        "            # SPK発生率グラフ\n",
        "            if not score_summary.empty:\n",
        "                plt.figure(figsize=(max(6, score_summary[current_score_col].nunique() * 0.8), 4))\n",
        "                barplot = sns.barplot(x=current_score_col, y='SPK_Rate', data=score_summary, palette=\"magma\", order = sorted(score_summary[current_score_col].unique()))\n",
        "                plt.title(f\"SPK Incidence Rate per Score Point ({desc_str})\")\n",
        "                plt.xlabel(\"Score Point\"); plt.ylabel(\"SPK Incidence Rate\")\n",
        "                plt.ylim(0, max(1.05, score_summary['SPK_Rate'].max() * 1.1 if not score_summary['SPK_Rate'].empty else 1.05))\n",
        "                for i, patch in enumerate(barplot.patches): # enumerate(barplot.patches) の方が安全\n",
        "                    try:\n",
        "                        # X軸のカテゴリ名（スコアポイント）を正しく取得\n",
        "                        # barplot.get_xticklabels() から取得するか、orderリストを参照\n",
        "                        score_value_at_bar = sorted(score_summary[current_score_col].unique())[i]\n",
        "                        rate_percent = score_summary[score_summary[current_score_col] == score_value_at_bar]['SPK_Rate_Percent'].iloc[0]\n",
        "                        bar_x_pos = patch.get_x() + patch.get_width() / 2\n",
        "                        barplot.text(bar_x_pos, patch.get_height() + 0.02, f\"{rate_percent:.1f}%\", color='black', ha=\"center\", va=\"bottom\")\n",
        "                    except (IndexError, KeyError) as e_bar:\n",
        "                        print(f\"Warning: Could not annotate bar for score (index {i}): {e_bar}\")\n",
        "                plt.tight_layout()\n",
        "                rate_fig_path = os.path.join(output_dir, f\"Rateplot_Fixed_{desc_str.replace(' ','_')}_Score.png\")\n",
        "                plt.savefig(rate_fig_path, dpi=300); print(f\"SPK rate plot saved to {rate_fig_path}\"); plt.show()\n",
        "    else:\n",
        "        print(f\"Not enough score variation to display score distribution for {desc_str}.\")\n",
        "\n",
        "    print(f\"--- Evaluation for {desc_str} complete ---\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. メイン評価処理\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n--- 5. Main Evaluation Process ---\")\n",
        "\n",
        "# --- 5a. 術前スコアリングシステムの評価 ---\n",
        "if pre_operative_factors_valid:\n",
        "    evaluate_fixed_factor_system(df_analysis, target_col,\n",
        "                                 pre_operative_factors_valid, scoring_rules,\n",
        "                                 desc_str=\"Pre-operative\")\n",
        "else:\n",
        "    print(\"\\nNo valid pre-operative factors to evaluate.\")\n",
        "\n",
        "# --- 5b. 術後スコアリングシステムの評価 ---\n",
        "if post_operative_factors_valid:\n",
        "    evaluate_fixed_factor_system(df_analysis, target_col,\n",
        "                                 post_operative_factors_valid, scoring_rules,\n",
        "                                 desc_str=\"Post-operative\")\n",
        "else:\n",
        "    print(\"\\nNo valid post-operative factors to evaluate.\")\n",
        "\n",
        "print(\"\\n\\nAnalysis Complete.\")\n",
        "print(f\"Check '{output_dir}' for any generated plots.\")"
      ],
      "metadata": {
        "id": "LSRBu9tXWmBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# 眼瞼下垂術後SPKリスク解析：スコアリング自動探索と評価 Ver.3 (同率首位対応・重複ルール統合)\n",
        "# =============================================================\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 0. ライブラリのインポートと基本設定\n",
        "# -------------------------------------------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from itertools import combinations, product # product を追加\n",
        "from tqdm.auto import tqdm\n",
        "import joblib\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.sm_exceptions import PerfectSeparationError, ConvergenceWarning\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from IPython.display import display\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
        "warnings.filterwarnings('ignore', message=\"Maximum Likelihood optimization failed to converge\")\n",
        "warnings.filterwarnings('ignore', message=\"Method 'first' requires numeric dtype to function correctly\")\n",
        "print(\"Libraries imported.\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1. Google Drive マウント と ファイル/フォルダ設定\n",
        "# -------------------------------------------------------------\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    # ★★★ Google Drive内のExcelファイルのパスを指定してください ★★★\n",
        "    file_path = \"/content/drive/Shareddrives/岩崎Dr_IgG4 deulk/眼瞼下垂ドライアイ/眼瞼下垂★ドライアイ_kit.xlsx\"\n",
        "    # ★★★ 結果を出力するフォルダを指定してください ★★★\n",
        "    output_dir = \"/content/super_flexible_scoring_eval_v3_ties_dedup_top10preop\" # 新しいフォルダ名\n",
        "    print(\"Google Drive mounted.\")\n",
        "except ModuleNotFoundError:\n",
        "    print(\"Running in a non-Colab environment.\")\n",
        "    file_path = \"眼瞼下垂★ドライアイ_kit.xlsx\"; # 例\n",
        "    output_dir = \"./super_flexible_scoring_eval_v3_ties_dedup_top10preop\"\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True); print(f\"Output directory: {output_dir}\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2. データ読み込みと前処理\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 2. Loading and Preprocessing Data ---\")\n",
        "try:\n",
        "    df_all = pd.read_excel(file_path, sheet_name=\"対象症例sides\", header=0)\n",
        "    print(f\"Excel file loaded. Shape: {df_all.shape}\")\n",
        "except FileNotFoundError: print(f\"Error: File not found: {file_path}\"); exit()\n",
        "except Exception as e: print(f\"Error loading Excel: {e}\"); exit()\n",
        "cols_to_convert = [\"MRD-1 pre\", \"MRD-1 3M\", \"MRD-2 pre\", \"MRD-2 3M\", \"levator_function pre\", \"BUT pre\", \"BUT post\", \"SPK pre\", \"SPK post\"]\n",
        "for col in cols_to_convert:\n",
        "    if col in df_all.columns: df_all[col] = pd.to_numeric(df_all[col], errors='coerce')\n",
        "if \"MRD-1 3M\" in df_all.columns and \"MRD-1 pre\" in df_all.columns: df_all[\"ΔMRD-1\"] = df_all[\"MRD-1 3M\"] - df_all[\"MRD-1 pre\"]\n",
        "else: df_all[\"ΔMRD-1\"] = np.nan\n",
        "if \"MRD-2 3M\" in df_all.columns and \"MRD-2 pre\" in df_all.columns: df_all[\"ΔMRD-2\"] = df_all[\"MRD-2 3M\"] - df_all[\"MRD-2 pre\"]\n",
        "else: df_all[\"ΔMRD-2\"] = np.nan\n",
        "if \"SPK pre\" not in df_all.columns: print(\"Error: 'SPK pre' column not found.\"); exit()\n",
        "df = df_all[df_all[\"SPK pre\"] == 0].copy()\n",
        "if \"SPK post\" not in df.columns: print(\"Error: 'SPK post' column not found.\"); exit()\n",
        "df_analysis = df[df['SPK post'].isin([0, 1])].copy()\n",
        "df_analysis['SPK post'] = df_analysis['SPK post'].astype(int)\n",
        "target_col = 'SPK post'\n",
        "if df_analysis.empty or df_analysis[target_col].nunique() < 2: print(\"Error: Insufficient data for analysis.\"); exit()\n",
        "print(f\"Analysis data shape: {df_analysis.shape}, Target distribution:\\n{df_analysis[target_col].value_counts(normalize=True).round(3)}\")\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 3. スコアリングルール生成関数と候補変数定義\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 3. Generating Flexible Scoring Rule Sets ---\")\n",
        "def generate_flexible_scoring_rule_sets():\n",
        "    all_rule_sets = {}\n",
        "    rule_set_id_counter = 1\n",
        "    # --- 各因子のルールバリエーションを定義 ---\n",
        "    mrd1_pre_rules_options = {\n",
        "        \"mrd1pre_le0.5_2pt\": {\"MRD-1 pre\": {\"condition\": lambda x: pd.notna(x) and x <= 0.5, \"points\": 2, \"description\": \"MRD-1 pre <= 0.5 mm (2pt)\"}},\n",
        "        \"mrd1pre_none\": {}\n",
        "    }\n",
        "    delta_mrd1_rules_options = {\n",
        "        \"dmrd1_ge2.0_1pt\": {\"ΔMRD-1\": {\"condition\": lambda x: pd.notna(x) and x >= 2.0, \"points\": 1, \"description\": \"ΔMRD-1 >= 2.0 mm (1pt)\"}},\n",
        "        \"dmrd1_ge2.5_1pt\": {\"ΔMRD-1\": {\"condition\": lambda x: pd.notna(x) and x >= 2.5, \"points\": 1, \"description\": \"ΔMRD-1 >= 2.5 mm (1pt)\"}},\n",
        "        #\"dmrd1_ge3.0_1pt\": {\"ΔMRD-1\": {\"condition\": lambda x: pd.notna(x) and x >= 3.0, \"points\": 1, \"description\": \"ΔMRD-1 >= 3.0 mm (1pt)\"}},\n",
        "        #\"dmrd1_tiered_2_3\": {\"ΔMRD-1\": {\"condition\": lambda x: pd.notna(x) and x >= 2.0, \"points_logic\": lambda x: 2 if x >= 3.0 else 1, \"points_display\": \"1-2\", \"description\": \"ΔMRD-1 [2-2.9mm:1pt, >=3mm:2pt]\"}},\n",
        "        #\"dmrd1_none\": {}\n",
        "    }\n",
        "    levator_rules_options = {\n",
        "        \"lev_lt7_1pt\": {\"levator_function pre\": {\"condition\": lambda x: pd.notna(x) and x < 7.0, \"points\": 1, \"description\": \"Levator < 7.0 mm (1pt)\"}},\n",
        "        \"lev_lt8_1pt\": {\"levator_function pre\": {\"condition\": lambda x: pd.notna(x) and x < 8.0, \"points\": 1, \"description\": \"Levator < 8.0 mm (1pt)\"}},\n",
        "        \"lev_lt9_1pt\": {\"levator_function pre\": {\"condition\": lambda x: pd.notna(x) and x < 9.0, \"points\": 1, \"description\": \"Levator < 9.0 mm (1pt)\"}},\n",
        "        \"lev_none\": {}\n",
        "    }\n",
        "    but_pre_rules_options = {\n",
        "        #\"butpre_lt4_1pt\": {\"BUT pre\": {\"condition\": lambda x: pd.notna(x) and x < 4.0, \"points\": 1, \"description\": \"BUT pre < 4.0s (1pt)\"}},\n",
        "        \"butpre_lt5_1pt\": {\"BUT pre\": {\"condition\": lambda x: pd.notna(x) and x < 5.0, \"points\": 1, \"description\": \"BUT pre < 5.0s (1pt)\"}},\n",
        "        \"butpre_lt6_1pt\": {\"BUT pre\": {\"condition\": lambda x: pd.notna(x) and x < 6.0, \"points\": 1, \"description\": \"BUT pre < 6.0s (1pt)\"}},\n",
        "        \"butpre_none\": {}\n",
        "    }\n",
        "    but_post_rules_options = {\n",
        "        #\"butpost_lt3_1pt\": {\"BUT post\": {\"condition\": lambda x: pd.notna(x) and x < 3.0, \"points\": 1, \"description\": \"BUT post < 3.0s (1pt)\"}},\n",
        "        \"butpost_lt4_1pt\": {\"BUT post\": {\"condition\": lambda x: pd.notna(x) and x < 4.0, \"points\": 1, \"description\": \"BUT post < 4.0s (1pt)\"}},\n",
        "        \"butpost_lt5_1pt\": {\"BUT post\": {\"condition\": lambda x: pd.notna(x) and x < 5.0, \"points\": 1, \"description\": \"BUT post < 5.0s (1pt)\"}},\n",
        "        \"butpost_none\": {}\n",
        "    }\n",
        "    rule_variation_source_lists = [list(mrd1_pre_rules_options.items()), list(delta_mrd1_rules_options.items()), list(levator_rules_options.items()), list(but_pre_rules_options.items()), list(but_post_rules_options.items())]\n",
        "    for rule_combination_tuple in product(*rule_variation_source_lists):\n",
        "        current_ruleset_dict = {}; ruleset_name_parts_list = []; has_at_least_one_active_rule = False\n",
        "        for name_key_str, rule_def_dict in rule_combination_tuple:\n",
        "            current_ruleset_dict.update(rule_def_dict)\n",
        "            if rule_def_dict: ruleset_name_parts_list.append(name_key_str); has_at_least_one_active_rule = True\n",
        "        if not has_at_least_one_active_rule : continue\n",
        "        ruleset_name_str = f\"RS{rule_set_id_counter}_\" + \"_\".join(ruleset_name_parts_list) if ruleset_name_parts_list else f\"RS{rule_set_id_counter}_NoOptionalFactors\"\n",
        "        has_valid_rule = any(current_ruleset_dict.get(factor) for factor in [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT pre\", \"BUT post\"])\n",
        "        if not has_valid_rule: continue # 少なくとも1つの評価対象因子ルールがないものは除外\n",
        "        all_rule_sets[ruleset_name_str] = current_ruleset_dict; rule_set_id_counter += 1\n",
        "    print(f\"Generated {len(all_rule_sets)} flexible scoring rule sets to evaluate.\")\n",
        "    return all_rule_sets\n",
        "generated_flexible_rule_sets = generate_flexible_scoring_rule_sets()\n",
        "all_possible_score_factors = [\"MRD-1 pre\", \"ΔMRD-1\", \"levator_function pre\", \"BUT pre\", \"BUT post\"]\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 4. スコアリングシステム評価関数\n",
        "# -------------------------------------------------------------\n",
        "def evaluate_scoring_systems_flexible(df_input, target_variable, candidate_factors_list, rules_dict, desc_str=\"\"):\n",
        "    score_system_results_list = []\n",
        "    actual_candidates = [f for f in candidate_factors_list if f in rules_dict] # ルール定義がある因子のみ\n",
        "    if not actual_candidates : return pd.DataFrame() # 評価対象因子がない場合は空のDFを返す\n",
        "\n",
        "    for k in tqdm(range(1, len(actual_candidates) + 1), desc=f\"Scoring Systems ({desc_str})\", leave=False):\n",
        "        for current_factor_subset_tuple in combinations(actual_candidates, k):\n",
        "            current_factors_in_subset = list(current_factor_subset_tuple)\n",
        "            if not current_factors_in_subset: continue\n",
        "\n",
        "            def calculate_total_score_for_row(row_data, selected_vars, defined_rules):\n",
        "                total_score_val = 0\n",
        "                for var_name_in_subset in selected_vars:\n",
        "                    if var_name_in_subset in defined_rules and var_name_in_subset in row_data: # Ensure factor is in data\n",
        "                        rule = defined_rules[var_name_in_subset]\n",
        "                        # Check if data for condition is not NaN before applying condition\n",
        "                        if pd.notna(row_data[var_name_in_subset]) and rule[\"condition\"](row_data[var_name_in_subset]):\n",
        "                            if \"points_logic\" in rule: total_score_val += rule[\"points_logic\"](row_data[var_name_in_subset])\n",
        "                            else: total_score_val += rule[\"points\"]\n",
        "                return total_score_val\n",
        "\n",
        "            temp_df_with_score = df_input.copy(); score_col_name = \"temp_total_score\"\n",
        "            temp_df_with_score[score_col_name] = temp_df_with_score.apply(lambda r: calculate_total_score_for_row(r, current_factors_in_subset, defined_rules=rules_dict), axis=1)\n",
        "\n",
        "            df_model_eval_current = temp_df_with_score[[target_variable, score_col_name]].dropna()\n",
        "\n",
        "            if df_model_eval_current.shape[0] < 10 or df_model_eval_current[target_variable].nunique() < 2 or df_model_eval_current[score_col_name].nunique() < 2:\n",
        "                score_system_results_list.append({'score_factors': ', '.join(current_factors_in_subset), 'num_score_factors': len(current_factors_in_subset), 'training_auc_of_score_model': np.nan, 'model_object':None, 'error': 'Insufficient data or no score variation'})\n",
        "                continue\n",
        "\n",
        "            y_eval_current = df_model_eval_current[target_variable]; X_eval_score_current = sm.add_constant(df_model_eval_current[[score_col_name]])\n",
        "            auc_val_current = np.nan; error_msg_current = None; model_fit_current = None\n",
        "            try:\n",
        "                score_logit_model = sm.Logit(y_eval_current, X_eval_score_current); model_fit_current = score_logit_model.fit(disp=False, maxiter=100, warn_convergence=False)\n",
        "                if model_fit_current.mle_retvals['converged']:\n",
        "                    y_pred_prob_score = model_fit_current.predict(X_eval_score_current); auc_val_current = roc_auc_score(y_eval_current, y_pred_prob_score)\n",
        "                else: error_msg_current = \"Convergence Failed\"\n",
        "            except PerfectSeparationError: error_msg_current = \"Perfect Separation\"\n",
        "            except Exception as e_current: error_msg_current = str(e_current)\n",
        "\n",
        "            score_system_results_list.append({'score_factors': ', '.join(current_factors_in_subset), 'num_score_factors': len(current_factors_in_subset), 'training_auc_of_score_model': auc_val_current, 'model_object': model_fit_current, 'error': error_msg_current})\n",
        "\n",
        "    results_df_current = pd.DataFrame(score_system_results_list).sort_values(by='training_auc_of_score_model', ascending=False, na_position='last').reset_index(drop=True)\n",
        "    return results_df_current\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 5. メイン処理: 各ルールセットでの評価と最終選択\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n--- 5. Main Processing: Evaluating Flexible Rule Sets and Final Selection ---\")\n",
        "N_SPLITS_CV = 5\n",
        "TOP_N_SCORING_SYSTEMS_PER_RULESET = 3\n",
        "FINAL_TOP_N_TO_DISPLAY = 10 # 最終的に表示するユニークなシステムの数\n",
        "cv_pipeline_score = Pipeline([('scaler', StandardScaler()), ('logistic', LogisticRegression(solver='liblinear', random_state=42, class_weight='balanced'))])\n",
        "cv_splitter_score = StratifiedKFold(n_splits=N_SPLITS_CV, shuffle=True, random_state=42)\n",
        "all_best_systems_across_rulesets = []\n",
        "\n",
        "for ruleset_name, current_scoring_rules in tqdm(generated_flexible_rule_sets.items(), desc=\"Processing Rule Sets\"):\n",
        "    # --- PreOp ---\n",
        "    # current_scoring_rules にキーが存在し、かつ df_analysis のカラムにも存在する因子のみを候補とする\n",
        "    current_preop_candidates_for_ruleset = [\n",
        "        f for f in all_possible_score_factors\n",
        "        if f in current_scoring_rules and \"BUT post\" not in f and f in df_analysis.columns\n",
        "    ]\n",
        "    if current_preop_candidates_for_ruleset: # 候補因子が実際に存在する場合のみ評価\n",
        "        preop_scoring_results_df = evaluate_scoring_systems_flexible(df_analysis, target_col, current_preop_candidates_for_ruleset, current_scoring_rules, desc_str=f\"PreOp-{ruleset_name[:15]}\")\n",
        "        top_n_preop_scores = preop_scoring_results_df[preop_scoring_results_df['error'].isna() & preop_scoring_results_df['training_auc_of_score_model'].notna()].head(TOP_N_SCORING_SYSTEMS_PER_RULESET)\n",
        "        if not top_n_preop_scores.empty:\n",
        "            for index, row_data in top_n_preop_scores.iterrows():\n",
        "                current_factors_list = row_data['score_factors'].split(', '); temp_df_cv = df_analysis.copy()\n",
        "                # スコア計算部分を修正\n",
        "                def calculate_cv_score(r_cv, factors, rules):\n",
        "                    score = 0\n",
        "                    for f_cv in factors:\n",
        "                        if f_cv in rules and f_cv in r_cv and pd.notna(r_cv[f_cv]):\n",
        "                            rule_detail = rules[f_cv]\n",
        "                            if rule_detail['condition'](r_cv[f_cv]):\n",
        "                                if 'points_logic' in rule_detail: score += rule_detail['points_logic'](r_cv[f_cv])\n",
        "                                else: score += rule_detail['points']\n",
        "                    return score\n",
        "                temp_df_cv['current_total_score'] = temp_df_cv.apply(lambda r_cv: calculate_cv_score(r_cv, current_factors_list, current_scoring_rules), axis=1)\n",
        "\n",
        "                df_cv_eval_current = temp_df_cv[[target_col, 'current_total_score']].dropna();\n",
        "                y_cv_current = df_cv_eval_current[target_col]; X_cv_score_current = df_cv_eval_current[['current_total_score']]\n",
        "                mean_auc_current = np.nan; std_auc_current = np.nan\n",
        "                if X_cv_score_current.shape[0] >= N_SPLITS_CV and y_cv_current.nunique() == 2 and X_cv_score_current['current_total_score'].nunique() >=2 :\n",
        "                    try:\n",
        "                        cv_scores_current = cross_val_score(cv_pipeline_score, X_cv_score_current, y_cv_current, cv=cv_splitter_score, scoring='roc_auc', n_jobs=-1, error_score=np.nan) # error_score='raise' -> np.nan\n",
        "                        if np.all(np.isfinite(cv_scores_current)) and len(cv_scores_current) == N_SPLITS_CV : mean_auc_current = np.nanmean(cv_scores_current); std_auc_current = np.nanstd(cv_scores_current)\n",
        "                    except ValueError: pass # Catches \"Found input variables with inconsistent numbers of samples\" or other CV errors\n",
        "                    except Exception: pass # Broader catch for other unexpected CV issues\n",
        "                all_best_systems_across_rulesets.append({'score_factors': row_data['score_factors'], 'num_score_factors': row_data['num_score_factors'], 'training_auc': row_data['training_auc_of_score_model'], 'mean_cv_auc': mean_auc_current, 'std_cv_auc': std_auc_current, 'rule_set_name': ruleset_name, 'model_type': 'Pre-operative', 'model_object_from_search': row_data['model_object']})\n",
        "\n",
        "    # --- PostOp ---\n",
        "    current_postop_candidates_for_ruleset = [\n",
        "        f for f in all_possible_score_factors\n",
        "        if f in current_scoring_rules and f in df_analysis.columns\n",
        "    ]\n",
        "    if current_postop_candidates_for_ruleset:\n",
        "        postop_scoring_results_df = evaluate_scoring_systems_flexible(df_analysis, target_col, current_postop_candidates_for_ruleset, current_scoring_rules, desc_str=f\"PostOp-{ruleset_name[:15]}\")\n",
        "        top_n_postop_scores = postop_scoring_results_df[postop_scoring_results_df['error'].isna() & postop_scoring_results_df['training_auc_of_score_model'].notna()].head(TOP_N_SCORING_SYSTEMS_PER_RULESET)\n",
        "        if not top_n_postop_scores.empty:\n",
        "            for index, row_data in top_n_postop_scores.iterrows():\n",
        "                current_factors_list = row_data['score_factors'].split(', '); temp_df_cv = df_analysis.copy()\n",
        "                # スコア計算部分を修正 (PreOpと同様)\n",
        "                temp_df_cv['current_total_score'] = temp_df_cv.apply(lambda r_cv: calculate_cv_score(r_cv, current_factors_list, current_scoring_rules), axis=1)\n",
        "\n",
        "                df_cv_eval_current = temp_df_cv[[target_col, 'current_total_score']].dropna();\n",
        "                y_cv_current = df_cv_eval_current[target_col]; X_cv_score_current = df_cv_eval_current[['current_total_score']]\n",
        "                mean_auc_current = np.nan; std_auc_current = np.nan\n",
        "                if X_cv_score_current.shape[0] >= N_SPLITS_CV and y_cv_current.nunique() == 2 and X_cv_score_current['current_total_score'].nunique() >=2 :\n",
        "                    try:\n",
        "                        cv_scores_current = cross_val_score(cv_pipeline_score, X_cv_score_current, y_cv_current, cv=cv_splitter_score, scoring='roc_auc', n_jobs=-1, error_score=np.nan)\n",
        "                        if np.all(np.isfinite(cv_scores_current)) and len(cv_scores_current) == N_SPLITS_CV: mean_auc_current = np.nanmean(cv_scores_current); std_auc_current = np.nanstd(cv_scores_current)\n",
        "                    except ValueError: pass\n",
        "                    except Exception: pass\n",
        "                all_best_systems_across_rulesets.append({'score_factors': row_data['score_factors'], 'num_score_factors': row_data['num_score_factors'], 'training_auc': row_data['training_auc_of_score_model'], 'mean_cv_auc': mean_auc_current, 'std_cv_auc': std_auc_current, 'rule_set_name': ruleset_name, 'model_type': 'Post-operative', 'model_object_from_search': row_data['model_object']})\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 6. 全ルールセットを通した最終ベストモデルの選択と評価 (同率首位対応・重複ルール統合修正)\n",
        "# -------------------------------------------------------------\n",
        "print(\"\\n\\n======================================================\")\n",
        "print(f\"=== Overall Best Scoring Systems (Selected by Best Mean {N_SPLITS_CV}-Fold CV AUC across all Rule Sets) ===\")\n",
        "print(\"======================================================\")\n",
        "\n",
        "def display_final_system_evaluation(system_info, df_analysis_data, target_col_name, all_rulesets_dict, desc_str=\"\"):\n",
        "    if system_info is None: print(f\"\\nNo overall best {desc_str} scoring system selected.\"); return\n",
        "    if not isinstance(system_info, dict) or 'score_factors' not in system_info or 'rule_set_name' not in system_info:\n",
        "        print(f\"Warning: Invalid system_info provided for {desc_str}. Skipping display.\"); return\n",
        "\n",
        "    print(f\"\\n--- Overall Best {desc_str} Scoring System (Representative) ---\")\n",
        "    final_selected_score_factors = system_info['score_factors'].split(', ')\n",
        "    final_rules_for_set = all_rulesets_dict.get(system_info['rule_set_name'], {})\n",
        "\n",
        "    print(f\"Original Rule Set Used for this representative: {system_info['rule_set_name']}\")\n",
        "    print(f\"Selected Score Factors ({system_info.get('num_score_factors', len(final_selected_score_factors))}): {', '.join(final_selected_score_factors)}\")\n",
        "    print(f\"Mean {N_SPLITS_CV}-Fold CV AUC: {system_info.get('mean_cv_auc', np.nan):.4f} (+/- {system_info.get('std_cv_auc', np.nan):.4f})\")\n",
        "    print(f\"Original Training AUC of Score Model: {system_info.get('training_auc', np.nan):.4f}\")\n",
        "\n",
        "    final_df_display = df_analysis_data.copy()\n",
        "\n",
        "    def calculate_final_score_for_row(row_data, selected_factors, rules_dict_for_set):\n",
        "        score_val = 0\n",
        "        for factor_name in selected_factors:\n",
        "            if factor_name in rules_dict_for_set and factor_name in row_data and pd.notna(row_data[factor_name]):\n",
        "                rule_detail = rules_dict_for_set[factor_name]\n",
        "                if rule_detail['condition'](row_data[factor_name]):\n",
        "                    if 'points_logic' in rule_detail:\n",
        "                        score_val += rule_detail['points_logic'](row_data[factor_name])\n",
        "                    else:\n",
        "                        score_val += rule_detail['points']\n",
        "        return score_val\n",
        "\n",
        "    final_df_display['final_score'] = final_df_display.apply(\n",
        "        lambda r_disp: calculate_final_score_for_row(r_disp, final_selected_score_factors, final_rules_for_set), axis=1\n",
        "    )\n",
        "    df_final_model_eval_disp = final_df_display[[target_col_name, 'final_score']].dropna()\n",
        "\n",
        "    # Create a unique identifier for plot filenames based on factors and rules (simplified)\n",
        "    rule_sig_for_filename = system_info.get('rule_signature', 'UnknownRule').replace('|','_').replace(':','-')[:50] # Truncate for safety\n",
        "    plot_suffix = f\"{desc_str.replace(' ','_')}_{rule_sig_for_filename}\"\n",
        "\n",
        "    if not df_final_model_eval_disp.empty and df_final_model_eval_disp['final_score'].nunique() > 0:\n",
        "        print(f\"\\n--- Score Distribution and SPK Incidence for Best {desc_str} Scoring System ---\")\n",
        "        score_summary_disp = df_final_model_eval_disp.groupby('final_score')[target_col_name].agg(Total_Cases='count', SPK_Positive_Cases='sum', SPK_Rate='mean').reset_index()\n",
        "        score_summary_disp['SPK_Rate_Percent'] = (score_summary_disp['SPK_Rate'] * 100).round(1); display(score_summary_disp)\n",
        "\n",
        "        count_fig_name = f\"Count_{plot_suffix}.png\"\n",
        "        rate_fig_name = f\"Rate_{plot_suffix}.png\"\n",
        "        roc_fig_name = f\"ROC_{plot_suffix}.png\"\n",
        "\n",
        "        plt.figure(figsize=(max(6, score_summary_disp['final_score'].nunique() * 0.8), 4)); sns.countplot(x='final_score', data=df_final_model_eval_disp, palette=\"viridis\", order = sorted(df_final_model_eval_disp['final_score'].unique()))\n",
        "        plt.title(f\"Case Count ({desc_str} System)\"); plt.xlabel(\"Score\"); plt.ylabel(\"Cases\"); plt.tight_layout(); count_fig_path_disp = os.path.join(output_dir, count_fig_name); plt.savefig(count_fig_path_disp, dpi=300); print(f\"Plot saved: {count_fig_path_disp}\"); plt.show()\n",
        "        if not score_summary_disp.empty:\n",
        "            plt.figure(figsize=(max(6, score_summary_disp['final_score'].nunique() * 0.8), 4)); barplot = sns.barplot(x='final_score', y='SPK_Rate', data=score_summary_disp, palette=\"magma\", order = sorted(score_summary_disp['final_score'].unique()))\n",
        "            plt.title(f\"SPK Rate ({desc_str} System)\"); plt.xlabel(\"Score\"); plt.ylabel(\"SPK Rate\"); plt.ylim(0, max(1.05, score_summary_disp['SPK_Rate'].max() * 1.1 if not score_summary_disp['SPK_Rate'].empty else 1.05))\n",
        "            for i_disp, patch_disp in enumerate(barplot.patches):\n",
        "                 try:\n",
        "                     current_score_val_disp = sorted(score_summary_disp['final_score'].unique())[i_disp]; rate_percent_disp = score_summary_disp[score_summary_disp['final_score'] == current_score_val_disp]['SPK_Rate_Percent'].iloc[0]\n",
        "                     bar_x_pos_disp = patch_disp.get_x() + patch_disp.get_width() / 2; barplot.text(bar_x_pos_disp, patch_disp.get_height() + 0.02, f\"{rate_percent_disp:.1f}%\", color='black', ha=\"center\", va=\"bottom\")\n",
        "                 except (IndexError, KeyError): print(f\"Warning: Could not annotate bar for score (index {i_disp}, score {current_score_val_disp}).\")\n",
        "            plt.tight_layout(); rate_fig_path_disp = os.path.join(output_dir, rate_fig_name); plt.savefig(rate_fig_path_disp, dpi=300); print(f\"Plot saved: {rate_fig_path_disp}\"); plt.show()\n",
        "\n",
        "    if df_final_model_eval_disp.shape[0] > 10 and df_final_model_eval_disp[target_col_name].nunique() == 2 and df_final_model_eval_disp['final_score'].nunique() >=2 :\n",
        "        y_final_disp = df_final_model_eval_disp[target_col_name]; X_final_score_disp = sm.add_constant(df_final_model_eval_disp[['final_score']])\n",
        "        final_model_to_show = system_info.get('model_object_from_search')\n",
        "\n",
        "        try:\n",
        "            if final_model_to_show and hasattr(final_model_to_show, 'mle_retvals') and final_model_to_show.mle_retvals['converged']:\n",
        "                print(f\"\\n--- Summary (LogReg on Final {desc_str} Score - Stored Model) ---\")\n",
        "            else:\n",
        "                print(f\"\\n--- Summary (LogReg on Final {desc_str} Score - Refitting) ---\")\n",
        "                final_model_to_show = sm.Logit(y_final_disp, X_final_score_disp).fit(disp=False, maxiter=100, warn_convergence=False)\n",
        "\n",
        "            print(final_model_to_show.summary(xname=['Intercept', 'Final Score']))\n",
        "            print(\"\\nOdds Ratios (per 1 point score increase):\")\n",
        "            conf_final_disp = final_model_to_show.conf_int(); conf_final_disp['Odds Ratio'] = final_model_to_show.params\n",
        "            conf_final_disp.columns = ['CI 2.5%', 'CI 97.5%', 'Odds Ratio']; display(np.exp(conf_final_disp).round(3))\n",
        "\n",
        "            y_prob_final_disp = final_model_to_show.predict(X_final_score_disp)\n",
        "            fpr_disp, tpr_disp, _ = roc_curve(y_final_disp, y_prob_final_disp); auc_val_disp = roc_auc_score(y_final_disp, y_prob_final_disp)\n",
        "            plt.figure(figsize=(5,5))\n",
        "            plt.plot(fpr_disp, tpr_disp, label=f\"Final {desc_str} Score Model\\nAUC = {auc_val_disp:.3f}\")\n",
        "            plt.plot([0,1],[0,1],'k--'); plt.xlabel(\"1 - Specificity\"); plt.ylabel(\"Sensitivity\")\n",
        "            plt.title(f\"ROC Final {desc_str} Score\"); plt.legend(); plt.grid(True); plt.tight_layout()\n",
        "            roc_fig_path_disp = os.path.join(output_dir, roc_fig_name); plt.savefig(roc_fig_path_disp, dpi=300); print(f\"ROC saved: {roc_fig_path_disp}\"); plt.show()\n",
        "        except Exception as e_final_disp: print(f\"Error displaying final {desc_str} score model summary/ROC: {e_final_disp}\")\n",
        "\n",
        "    print(f\"\\nRules for the Best {desc_str} Scoring System:\")\n",
        "    num_other_identical_systems = system_info.get('num_identical_systems', 1) - 1\n",
        "    if num_other_identical_systems > 0:\n",
        "        print(f\"(This rule combination was also found as optimal in {num_other_identical_systems} other initial rule set variations)\")\n",
        "\n",
        "    for factor_name_disp in final_selected_score_factors:\n",
        "        if final_rules_for_set and factor_name_disp in final_rules_for_set:\n",
        "            rule_detail_disp = final_rules_for_set[factor_name_disp]\n",
        "            print(f\"  - {rule_detail_disp['description']}\") # Display full description\n",
        "        else:\n",
        "            print(f\"  - {factor_name_disp}: Rule details not found in the representative rule set for this factor.\")\n",
        "\n",
        "\n",
        "def get_rule_signature(system_row_data, all_rulesets_collection):\n",
        "    factors_str_list = sorted(system_row_data['score_factors'].split(', '))\n",
        "    rules_for_this_set = all_rulesets_collection.get(system_row_data['rule_set_name'], {})\n",
        "    signature_parts_list = []\n",
        "    for factor_item in factors_str_list:\n",
        "        if factor_item in rules_for_this_set:\n",
        "            rule_details_item = rules_for_this_set[factor_item]\n",
        "            desc_item = rule_details_item.get('description', f\"cond_{factor_item}\")\n",
        "            # Points can be direct or from logic, stringify for robust signature\n",
        "            points_item = str(rule_details_item.get('points', rule_details_item.get('points_display', 'logic')))\n",
        "            signature_parts_list.append(f\"{factor_item}:{desc_item}:{points_item}\")\n",
        "        else:\n",
        "            signature_parts_list.append(f\"{factor_item}:RULE_DEF_MISSING\")\n",
        "    return \"|\".join(signature_parts_list)\n",
        "\n",
        "\n",
        "if not all_best_systems_across_rulesets:\n",
        "    print(\"No valid scoring systems found across any rule set.\")\n",
        "else:\n",
        "    overall_best_systems_df = pd.DataFrame(all_best_systems_across_rulesets)\n",
        "    overall_best_systems_df = overall_best_systems_df.dropna(subset=['mean_cv_auc']) # CV AUCがNaNのものは除外\n",
        "\n",
        "    if overall_best_systems_df.empty:\n",
        "        print(\"No systems remained after removing those with NaN CV AUC.\")\n",
        "    else:\n",
        "        overall_best_systems_df['rule_signature'] = overall_best_systems_df.apply(\n",
        "            lambda r: get_rule_signature(r, generated_flexible_rule_sets), axis=1\n",
        "        )\n",
        "        overall_best_systems_df = overall_best_systems_df.sort_values(\n",
        "            by=['mean_cv_auc', 'num_score_factors', 'rule_signature', 'rule_set_name'],\n",
        "            ascending=[False, True, True, True]\n",
        "        )\n",
        "\n",
        "        # カウント列を追加: このルールシグネチャがいくつのルールセットから生まれたか\n",
        "        overall_best_systems_df['num_identical_systems'] = overall_best_systems_df.groupby('rule_signature')['rule_set_name'].transform('count')\n",
        "\n",
        "        # 表示用のユニークなシステムリスト (シグネチャで重複排除、最初のものを代表とする)\n",
        "        print(f\"\\n--- Top {FINAL_TOP_N_TO_DISPLAY} Unique-Rule Scoring Systems Overall (Ranked by Mean CV AUC, then by num_score_factors) ---\")\n",
        "        temp_unique_display_df = overall_best_systems_df.drop_duplicates(subset=['rule_signature'], keep='first')\n",
        "        display(temp_unique_display_df[['rule_set_name', 'model_type', 'score_factors', 'num_score_factors', 'training_auc', 'mean_cv_auc', 'std_cv_auc', 'rule_signature', 'num_identical_systems']].head(FINAL_TOP_N_TO_DISPLAY).round(4))\n",
        "\n",
        "\n",
        "        # --- 術前モデルの同率首位処理 (重複ルール統合版) ---\n",
        "        preop_systems_df = overall_best_systems_df[overall_best_systems_df['model_type'] == 'Pre-operative'].copy()\n",
        "        if not preop_systems_df.empty:\n",
        "            # Sort all pre-op systems by CV AUC, then num_score_factors, then rule_signature\n",
        "            preop_systems_df = preop_systems_df.sort_values(\n",
        "                by=['mean_cv_auc', 'num_score_factors', 'rule_signature', 'rule_set_name'],\n",
        "                ascending=[False, True, True, True]\n",
        "            )\n",
        "            # Get unique rule signatures, keeping the best (first after sorting)\n",
        "            unique_top_preop_systems = preop_systems_df.drop_duplicates(subset=['rule_signature'], keep='first')\n",
        "\n",
        "            print(f\"\\n--- Top {FINAL_TOP_N_TO_DISPLAY} Unique Pre-operative Scoring Systems (Ranked by Mean CV AUC, then by num_score_factors) ---\")\n",
        "            display(unique_top_preop_systems[['rule_set_name', 'score_factors', 'num_score_factors', 'training_auc', 'mean_cv_auc', 'std_cv_auc', 'rule_signature', 'num_identical_systems']].head(FINAL_TOP_N_TO_DISPLAY).round(4))\n",
        "\n",
        "\n",
        "            # Display details for the best (potentially tied) pre-operative models\n",
        "            max_cv_auc_preop = unique_top_preop_systems['mean_cv_auc'].max() # Max CV AUC from unique systems\n",
        "            # Select systems from the unique list that are close to the max CV AUC\n",
        "            best_overall_preop_group_to_detail = unique_top_preop_systems[\n",
        "                np.isclose(unique_top_preop_systems['mean_cv_auc'], max_cv_auc_preop, atol=1e-5)\n",
        "            ].copy()\n",
        "            # This group is already sorted by num_score_factors, rule_signature, rule_set_name due to prior sorting\n",
        "\n",
        "            if not best_overall_preop_group_to_detail.empty:\n",
        "                print(f\"\\n--- Displaying Details for Best Unique Pre-operative Model(s) (Mean CV AUC ≈ {max_cv_auc_preop:.4f}) ---\")\n",
        "                for _, system_info_row_preop in best_overall_preop_group_to_detail.iterrows():\n",
        "                    display_final_system_evaluation(system_info_row_preop.to_dict(), df_analysis, target_col, generated_flexible_rule_sets, desc_str=\"Pre-operative\")\n",
        "            else: print(\"\\nNo best pre-operative model found after CV and deduplication for detailed display.\")\n",
        "        else: print(\"\\nNo pre-operative models evaluated or passed CV.\")\n",
        "\n",
        "        # --- 術後モデルの同率首位処理 (重複ルール統合版) ---\n",
        "        postop_systems_df = overall_best_systems_df[overall_best_systems_df['model_type'] == 'Post-operative'].copy()\n",
        "        if not postop_systems_df.empty:\n",
        "             # Sort all post-op systems\n",
        "            postop_systems_df = postop_systems_df.sort_values(\n",
        "                by=['mean_cv_auc', 'num_score_factors', 'rule_signature', 'rule_set_name'],\n",
        "                ascending=[False, True, True, True]\n",
        "            )\n",
        "            unique_top_postop_systems = postop_systems_df.drop_duplicates(subset=['rule_signature'], keep='first')\n",
        "\n",
        "            print(f\"\\n--- Top {FINAL_TOP_N_TO_DISPLAY} Unique Post-operative Scoring Systems (Ranked by Mean CV AUC, then by num_score_factors) ---\")\n",
        "            display(unique_top_postop_systems[['rule_set_name', 'score_factors', 'num_score_factors', 'training_auc', 'mean_cv_auc', 'std_cv_auc', 'rule_signature', 'num_identical_systems']].head(FINAL_TOP_N_TO_DISPLAY).round(4))\n",
        "\n",
        "            max_cv_auc_postop = unique_top_postop_systems['mean_cv_auc'].max()\n",
        "            best_overall_postop_group_to_detail = unique_top_postop_systems[\n",
        "                np.isclose(unique_top_postop_systems['mean_cv_auc'], max_cv_auc_postop, atol=1e-5)\n",
        "            ].copy()\n",
        "\n",
        "            if not best_overall_postop_group_to_detail.empty:\n",
        "                print(f\"\\n--- Displaying Details for Best Unique Post-operative Model(s) (Mean CV AUC ≈ {max_cv_auc_postop:.4f}) ---\")\n",
        "                for _, system_info_row_postop in best_overall_postop_group_to_detail.iterrows():\n",
        "                    display_final_system_evaluation(system_info_row_postop.to_dict(), df_analysis, target_col, generated_flexible_rule_sets, desc_str=\"Post-operative\")\n",
        "            else: print(\"\\nNo best post-operative model found after CV and deduplication for detailed display.\")\n",
        "        else: print(\"\\nNo post-operative models evaluated or passed CV.\")\n",
        "\n",
        "print(\"\\nReminder: The selected scoring system's performance and rules depend on the evaluated rule sets.\")\n",
        "print(f\"\\nAnalysis Complete. Check '{output_dir}' for plots.\")"
      ],
      "metadata": {
        "id": "gYwB4qKuWpN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "puMkemyVWpm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Uu7pINGLWtG6"
      }
    }
  ]
}